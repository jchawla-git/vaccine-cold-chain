{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Vaccine Cold Chain Monitoring The challenge healthcare industry is facing in very close time is to deliver a billion doses of the vaccine for COVID-19. A vaccine typically travels through multiple sites before being ready for shipment. Multiple companies are involve to manufacture the vaccines. Vaccines need to be handled at an optimal cool temperature (36-46 degrees F) and managed through the cold chain process. Variability in events both internally through the business processes and external events impacting the supply chain across multiple business entities coupled with data residing in multiple clouds presents challenges around anomaly detection that ultimately the vaccin developer is responsible to decision and act on by modifying business processes. A traditional architecture requires creating new interfaces, integration into systems, federation and organization of data into a data lake to be ready for any derivation of insights. Our demonstration today, showcases a developer ready modern architecture comprising of integrated middleware that are used as building blocks to deliver the end to end solution. Use of DevOps tooling and practices accelerates the solution by x factor.","title":"What you will learn"},{"location":"#vaccine-cold-chain-monitoring","text":"The challenge healthcare industry is facing in very close time is to deliver a billion doses of the vaccine for COVID-19. A vaccine typically travels through multiple sites before being ready for shipment. Multiple companies are involve to manufacture the vaccines. Vaccines need to be handled at an optimal cool temperature (36-46 degrees F) and managed through the cold chain process. Variability in events both internally through the business processes and external events impacting the supply chain across multiple business entities coupled with data residing in multiple clouds presents challenges around anomaly detection that ultimately the vaccin developer is responsible to decision and act on by modifying business processes. A traditional architecture requires creating new interfaces, integration into systems, federation and organization of data into a data lake to be ready for any derivation of insights. Our demonstration today, showcases a developer ready modern architecture comprising of integrated middleware that are used as building blocks to deliver the end to end solution. Use of DevOps tooling and practices accelerates the solution by x factor.","title":"Vaccine Cold Chain Monitoring"},{"location":"compendium/","text":"Compendium Cloud Pak For Integration Event Streams product documentation Event Driven reference architecture Cloud Pak for Application IBM Cloud Pak for Applications Kabanero architecture and workflows Cloud Pak for Data Data AI reference architecture Cloud Pak for Data product documentation Install CP4D on Openshift Getting stared with Cloud Pak for data Explore Knowledge Catalog's Data Governance Capabilities Model deployment with Watson Machine Learning Cloud Pak for Automation Garage solution engineering - Denim compute Digital Business Automation Architecture Center - streamlines business operations DBA - Garage team's cookbook Openshift Ansible CI/CD Gitops Our Git action to update gitops Kabanero Appsody Git action Tekton - kubernetes native pipeline tool Anomaly detection Romeo Kienzler anomaly detection article 1 Romeo Kienzler anomaly detection - Generating data for anomaly detection Romeo Kienzler anomaly detection with Deeplearning4j Romeo Kienzler anomaly detection using Apache SystemML Romeo Kienzler anomaly detection using Tensoflow and Keras","title":"Compendium"},{"location":"compendium/#compendium","text":"","title":"Compendium"},{"location":"compendium/#cloud-pak-for-integration","text":"Event Streams product documentation Event Driven reference architecture","title":"Cloud Pak For Integration"},{"location":"compendium/#cloud-pak-for-application","text":"IBM Cloud Pak for Applications Kabanero architecture and workflows","title":"Cloud Pak for Application"},{"location":"compendium/#cloud-pak-for-data","text":"Data AI reference architecture Cloud Pak for Data product documentation Install CP4D on Openshift Getting stared with Cloud Pak for data Explore Knowledge Catalog's Data Governance Capabilities Model deployment with Watson Machine Learning","title":"Cloud Pak for Data"},{"location":"compendium/#cloud-pak-for-automation","text":"Garage solution engineering - Denim compute Digital Business Automation Architecture Center - streamlines business operations DBA - Garage team's cookbook","title":"Cloud Pak for Automation"},{"location":"compendium/#openshift","text":"Ansible","title":"Openshift"},{"location":"compendium/#cicd","text":"Gitops Our Git action to update gitops Kabanero Appsody Git action Tekton - kubernetes native pipeline tool","title":"CI/CD"},{"location":"compendium/#anomaly-detection","text":"Romeo Kienzler anomaly detection article 1 Romeo Kienzler anomaly detection - Generating data for anomaly detection Romeo Kienzler anomaly detection with Deeplearning4j Romeo Kienzler anomaly detection using Apache SystemML Romeo Kienzler anomaly detection using Tensoflow and Keras","title":"Anomaly detection"},{"location":"analyze/predictive-maintenance/","text":"Reefer Container Predictive Maintenance In this section, we discuss how to build an analytic model using machine learning techniques from data coming from event store like kafka. We train the model with the help of historical data to predict whether maintenance is required for the reefer container at a certain point in time. You will learn how to simulate date for reefer, develop the predictive maintenance model, and integrate the model into an application. Introduction A reefer container is a refrigerated shipping container used to store or transport frozen or cold goods perishable items or goods that require temperature control. Reefers make an excellent, portable solution for short or long term storage and can be used to ship or truck goods over long distances as they can be plugged into the power station on ships or have it clipped on generators attached. Perishable products must be kept at a controlled temperature, from point of origin to delivery to retailer or pharmacy. The logistics industry refers to this as the \u201ccold chain\u201d and it encompasses both \u201creefers\u201d (refrigerated containers) as well as warehouses, distribution centers and the final storage or holding areas. Throughout this chain the risk of failure is ever-present, meaning there is always a possibility of cargo exceeding permissible or safe temperature levels, even if only briefly. For example, a truck might be stopped without power in desert heat, allowing temperatures in the reefer to rise. Then power is restored and the temperature in the container comes back down, but the product is damaged. When cargo with such as any of those items listed above are exposed to temperatures outside of prescribed limits it can be damaged. In some cases this is evident, such as with bananas, but in other situations, like the transport of vaccines, it may not be apparent that damage has occurred and the vaccine becomes ineffective. For some products, going over temperature, even only briefly, can reduce shelf life dramatically, incurring substantial costs when it cannot be sold. Organizations contracting to ship perishable products often specify the permissible temperature range. However, even if it is possible to show that product was exposed to conditions outside of those contracted, proving where it happened, and thus responsibility, can be much harder. Predictive maintenance problem statement If you want a good understanding of the problem space for predictive maintenance read, Yana Ageeva's article in toward data science . The success of predictive maintenance models depend on three main components: having the right data framing the problem appropriately evaluating the predictions properly From a methodology point of view the Data Scientist needs to address the following questions: What type of failure to consider and which one to predict? What kind of failure is happening? slow degradation or instantaneous failure? What could be the relation between a product characteristics and the failure? What kind of measure exist to assess the given characteristic? Which measurements correspond to good functioning and which ones correspond to failure? How often metrics are reported? What question the model should answer? What kind of output should the model give? How long in advance should the model be able to indicate that a failure will occur? What are the business impact to do not predict the failure? and predicting false negative failure? What is the expected accuracy? Reefer problem types There are multiple different potential issues that could happen to a refrigerator container. We are choosing to model the \"Sensor Malfunctions\" issue: Sensors in the refrigeration unit need to be calibrated and be continuously operational. An example of failure may come from the air sensor making inaccurate readings of temperatures, which leads to sploiled content. A potential reason may come from a faulty calibration, which can go unnoticed for a good time period. It may be difficult to know if there is an issue or not. The other common potential issues are: Fluid leaks, like engine oil, coolant liquid. The preassure sensors added to the circuit may help identify preassure lost over time. Faulty belts and hoses. Faulty calibration: A non-calibrated reefer can cool at a slower or faster rate than desired. Damaged Air Chute. Condenser Issues like broken or damaged coils, clamps or bolts missing, and leaks. Door Seals damaged. Blocked air passage: to keep the temperature homogenous inside the reefer. So the question we want to answer is: does the Reefer keep accurate temperature overtime between what is set versus what is measured? Modeling techniques The model uses the generated data from above scenarios: When the container's door is open for a longer time - this gives a false positive that maintainence is required. When sensors are malfunctioning, it records arbitrary readings. When the readings are normal. We have currently trained our model on 3000 datapoints from the three scenarios above. There are different modeling approach to tackle predictive maintenance: regression model classification to predict failure for a given time period classify anomalous behavior: classes are not known in advance. Normal operation is known. compute probability of failure over time Code execution The simulator continuosly generates container metrics, publishes it to Kafka and run the predictMaintainence.ipynb to predict if maintainence is sought at this point in time. Model description We are using Machine Learning supervised learning here. There are two types of supervised learning - 1) Classification: Predict a categorical response, 2) Regression: Predict a continuous response Linear regression Pros: 1) Fast 2) No tuning required 3) Highly interpretable 4) Well-understood Cons: 1) Unlikely to produce the best predictive accuracy 2) Presumes a linear relationship between the features and response 3) If the relationship is highly non-linear as with many scenarios, linear relationship will not effectively model the relationship and its prediction would not be accurate Naive Bayes classification Naive Bayes is a probabilistic classifier inspired by the Bayes theorem under a simple assumption which is the attributes are conditionally independent. The classification is conducted by deriving the maximum posterior which is the maximal P(Ci|X) with the above assumption applying to Bayes theorem. This assumption greatly reduces the computational cost by only counting the class distribution. Even though the assumption is not valid in most cases since the attributes are dependent, surprisingly Naive Bayes has able to perform impressively. Naive Bayes is a very simple algorithm to implement and good results have obtained in most cases. It can be easily scalable to larger datasets since it takes linear time, rather than by expensive iterative approximation as used for many other types of classifiers. Naive Bayes can suffer from a problem called the zero probability problem. When the conditional probability is zero for a particular attribute, it fails to give a valid prediction. This needs to be fixed explicitly using a Laplacian estimator. Model evaluation We are using Root Mean Squared Error (RMSE) for evaluating the model performance. Root Mean Squared Error (RMSE) is the square root of the mean of the squared errors. Classification does better here as the scenarion is more of a classification problem. References Understand Reefer container For modeling predictive maintenance we found this article from BigData Republique, on Medium, very interesting. PREDICTION OF TEMPERATURE INSIDE A REFRIGERATED CONTAINER IN THE PRESENCE OF PERISHABLE GOODS Temperature Monitoring During Transportation, Storage and Processing of Perishable Products Understanding machine learning classifiers","title":"Reefer Container Predictive Maintenance"},{"location":"analyze/predictive-maintenance/#reefer-container-predictive-maintenance","text":"In this section, we discuss how to build an analytic model using machine learning techniques from data coming from event store like kafka. We train the model with the help of historical data to predict whether maintenance is required for the reefer container at a certain point in time. You will learn how to simulate date for reefer, develop the predictive maintenance model, and integrate the model into an application.","title":"Reefer Container Predictive Maintenance"},{"location":"analyze/predictive-maintenance/#introduction","text":"A reefer container is a refrigerated shipping container used to store or transport frozen or cold goods perishable items or goods that require temperature control. Reefers make an excellent, portable solution for short or long term storage and can be used to ship or truck goods over long distances as they can be plugged into the power station on ships or have it clipped on generators attached. Perishable products must be kept at a controlled temperature, from point of origin to delivery to retailer or pharmacy. The logistics industry refers to this as the \u201ccold chain\u201d and it encompasses both \u201creefers\u201d (refrigerated containers) as well as warehouses, distribution centers and the final storage or holding areas. Throughout this chain the risk of failure is ever-present, meaning there is always a possibility of cargo exceeding permissible or safe temperature levels, even if only briefly. For example, a truck might be stopped without power in desert heat, allowing temperatures in the reefer to rise. Then power is restored and the temperature in the container comes back down, but the product is damaged. When cargo with such as any of those items listed above are exposed to temperatures outside of prescribed limits it can be damaged. In some cases this is evident, such as with bananas, but in other situations, like the transport of vaccines, it may not be apparent that damage has occurred and the vaccine becomes ineffective. For some products, going over temperature, even only briefly, can reduce shelf life dramatically, incurring substantial costs when it cannot be sold. Organizations contracting to ship perishable products often specify the permissible temperature range. However, even if it is possible to show that product was exposed to conditions outside of those contracted, proving where it happened, and thus responsibility, can be much harder.","title":"Introduction"},{"location":"analyze/predictive-maintenance/#predictive-maintenance-problem-statement","text":"If you want a good understanding of the problem space for predictive maintenance read, Yana Ageeva's article in toward data science . The success of predictive maintenance models depend on three main components: having the right data framing the problem appropriately evaluating the predictions properly From a methodology point of view the Data Scientist needs to address the following questions: What type of failure to consider and which one to predict? What kind of failure is happening? slow degradation or instantaneous failure? What could be the relation between a product characteristics and the failure? What kind of measure exist to assess the given characteristic? Which measurements correspond to good functioning and which ones correspond to failure? How often metrics are reported? What question the model should answer? What kind of output should the model give? How long in advance should the model be able to indicate that a failure will occur? What are the business impact to do not predict the failure? and predicting false negative failure? What is the expected accuracy?","title":"Predictive maintenance problem statement"},{"location":"analyze/predictive-maintenance/#reefer-problem-types","text":"There are multiple different potential issues that could happen to a refrigerator container. We are choosing to model the \"Sensor Malfunctions\" issue: Sensors in the refrigeration unit need to be calibrated and be continuously operational. An example of failure may come from the air sensor making inaccurate readings of temperatures, which leads to sploiled content. A potential reason may come from a faulty calibration, which can go unnoticed for a good time period. It may be difficult to know if there is an issue or not. The other common potential issues are: Fluid leaks, like engine oil, coolant liquid. The preassure sensors added to the circuit may help identify preassure lost over time. Faulty belts and hoses. Faulty calibration: A non-calibrated reefer can cool at a slower or faster rate than desired. Damaged Air Chute. Condenser Issues like broken or damaged coils, clamps or bolts missing, and leaks. Door Seals damaged. Blocked air passage: to keep the temperature homogenous inside the reefer. So the question we want to answer is: does the Reefer keep accurate temperature overtime between what is set versus what is measured?","title":"Reefer problem types"},{"location":"analyze/predictive-maintenance/#modeling-techniques","text":"The model uses the generated data from above scenarios: When the container's door is open for a longer time - this gives a false positive that maintainence is required. When sensors are malfunctioning, it records arbitrary readings. When the readings are normal. We have currently trained our model on 3000 datapoints from the three scenarios above. There are different modeling approach to tackle predictive maintenance: regression model classification to predict failure for a given time period classify anomalous behavior: classes are not known in advance. Normal operation is known. compute probability of failure over time","title":"Modeling techniques"},{"location":"analyze/predictive-maintenance/#code-execution","text":"The simulator continuosly generates container metrics, publishes it to Kafka and run the predictMaintainence.ipynb to predict if maintainence is sought at this point in time.","title":"Code execution"},{"location":"analyze/predictive-maintenance/#model-description","text":"We are using Machine Learning supervised learning here. There are two types of supervised learning - 1) Classification: Predict a categorical response, 2) Regression: Predict a continuous response","title":"Model description"},{"location":"analyze/predictive-maintenance/#linear-regression","text":"Pros: 1) Fast 2) No tuning required 3) Highly interpretable 4) Well-understood Cons: 1) Unlikely to produce the best predictive accuracy 2) Presumes a linear relationship between the features and response 3) If the relationship is highly non-linear as with many scenarios, linear relationship will not effectively model the relationship and its prediction would not be accurate","title":"Linear regression"},{"location":"analyze/predictive-maintenance/#naive-bayes-classification","text":"Naive Bayes is a probabilistic classifier inspired by the Bayes theorem under a simple assumption which is the attributes are conditionally independent. The classification is conducted by deriving the maximum posterior which is the maximal P(Ci|X) with the above assumption applying to Bayes theorem. This assumption greatly reduces the computational cost by only counting the class distribution. Even though the assumption is not valid in most cases since the attributes are dependent, surprisingly Naive Bayes has able to perform impressively. Naive Bayes is a very simple algorithm to implement and good results have obtained in most cases. It can be easily scalable to larger datasets since it takes linear time, rather than by expensive iterative approximation as used for many other types of classifiers. Naive Bayes can suffer from a problem called the zero probability problem. When the conditional probability is zero for a particular attribute, it fails to give a valid prediction. This needs to be fixed explicitly using a Laplacian estimator.","title":"Naive Bayes classification"},{"location":"analyze/predictive-maintenance/#model-evaluation","text":"We are using Root Mean Squared Error (RMSE) for evaluating the model performance. Root Mean Squared Error (RMSE) is the square root of the mean of the squared errors. Classification does better here as the scenarion is more of a classification problem.","title":"Model evaluation"},{"location":"analyze/predictive-maintenance/#references","text":"Understand Reefer container For modeling predictive maintenance we found this article from BigData Republique, on Medium, very interesting. PREDICTION OF TEMPERATURE INSIDE A REFRIGERATED CONTAINER IN THE PRESENCE OF PERISHABLE GOODS Temperature Monitoring During Transportation, Storage and Processing of Perishable Products Understanding machine learning classifiers","title":"References"},{"location":"analyze/ws-ml-dev/","text":"Watson Studio: developing the predictive model Cloud Pak for data integrates Watson Studio to develop manchine learning models and do feature engineering. In this chapter we are using two approaches to develop the model, one using AutoAI and one use notebook . The Data Steward has prepared a dataset by joining different datasources, but he did not pay attention to the column semantic for building a machine learning. So the Data Scientist will start to review and adapt the data. Data analysis and refinery The data scientist wants to do at least two things on the current data set: remove unnecessary features (the longitude and lattitude will not have any value to assess the sensor anomaly), and transform the problem to be a classification problem by adding a label column. Note that we could have model the problem using unsupervised learning and identify anomaly with clustering or anomaly detection. This will be done in the future to present a more realistic approach to this classical industrial problem. For that, we use the Data Refinery capability of Cloud Pak for Data: and then clean and shape the data to prepare for the model. For example remove columns like latitude, longitude, timestamp, _id, telemetry_id: To define a new column to be used as label, we use the Operation tab to add Conditional replace to apply some basic logic on the sensor columns by using two thresholds: test the value of the O2 to be less then a threshold (11) with a second condition on the CO2 to be greater than another threshold (12): We add the new label (Issue, NoIssue) in the Anomaly Flag column: Which translates to something like: \"Replaced values for AnomalyFlag: CARBON_DIOXIDE_LEVEL where value is greater than 12 as 'Issue', OXYGEN_LEVEL where value is less than 12 as 'Issue'. Replaced all remaining values with 'NoIssue'.\" Once the label column is added, and any new derived data are added, we can start a refinery job , that will create a new dataset in our project: AutoAI AutoAI uses data to automatically select the best supervised algorithm to determine the best classification or regression models with optimized hyper parameters. Within a project, we can add an Auto AI Experiment : Then specify a name and server configuration: Add a existing data source (the one prepared by the refinery job), and then specify the column to use for prediction (AnomalyFlag column): Then execute the prediction experiment. AutoAI will do different steps, split the data, prepare data, and then select model or algorithm that may better address the problem. For classification model, it will select among 30 potential candidates: decision tree, random forest, LGBM, XGBoost... Each algorithm selection will generate a pipeline which will be scored to present the most accurate one: The resulting experiments are ranked. When models or pipelines are created, we can see the details of each model, to see how they performed. The confusion matrix for the experiment ranked #1: The tool offers nice capabilities like the feature transformation: And the feature importance ranking, which helps to assess what are the features that impact the classification. Once the model is saved, it can be added to a Catalog, and then being deployed. Model Deployment Once a model is created is promoted to a \"space\". A space contains an overview of deployment status, the deployable assets, associated input and output data, and the associated environments. To access the deployment space, use the main left menu under Analyze -> Analytics deployment spaces . And under a given space we can see the assets and deployments in this space: On the Assets page, you can view the assets in the deployment space (Here we have our AutoAI experimental model deployed). You can see how many deployments each asset has and whether the asset is configured for performance monitoring. The following figure displays the deployed model And going into the deployed model view, we can see the API end point: And even test the prediction from the user interface. A next step is to infuse this model into the scoring application... Notebook In this section, we are developing a notebook on the telemetries dataset. We have already a project, and defined a data set from the data collection step , so we need to add a notebook. For that, in the project view, use Add to project ... and select the Notebook, specify a name and select python 3.6 runtime... Add a cell to access pandas, and maplablib in the first cell, and then get the code for accessing the dataset, by using the top right button: To Be Continued...","title":"Define the anomaly detection scoring model with Watson Studio"},{"location":"analyze/ws-ml-dev/#watson-studio-developing-the-predictive-model","text":"Cloud Pak for data integrates Watson Studio to develop manchine learning models and do feature engineering. In this chapter we are using two approaches to develop the model, one using AutoAI and one use notebook . The Data Steward has prepared a dataset by joining different datasources, but he did not pay attention to the column semantic for building a machine learning. So the Data Scientist will start to review and adapt the data.","title":"Watson Studio: developing the predictive model"},{"location":"analyze/ws-ml-dev/#data-analysis-and-refinery","text":"The data scientist wants to do at least two things on the current data set: remove unnecessary features (the longitude and lattitude will not have any value to assess the sensor anomaly), and transform the problem to be a classification problem by adding a label column. Note that we could have model the problem using unsupervised learning and identify anomaly with clustering or anomaly detection. This will be done in the future to present a more realistic approach to this classical industrial problem. For that, we use the Data Refinery capability of Cloud Pak for Data: and then clean and shape the data to prepare for the model. For example remove columns like latitude, longitude, timestamp, _id, telemetry_id: To define a new column to be used as label, we use the Operation tab to add Conditional replace to apply some basic logic on the sensor columns by using two thresholds: test the value of the O2 to be less then a threshold (11) with a second condition on the CO2 to be greater than another threshold (12): We add the new label (Issue, NoIssue) in the Anomaly Flag column: Which translates to something like: \"Replaced values for AnomalyFlag: CARBON_DIOXIDE_LEVEL where value is greater than 12 as 'Issue', OXYGEN_LEVEL where value is less than 12 as 'Issue'. Replaced all remaining values with 'NoIssue'.\" Once the label column is added, and any new derived data are added, we can start a refinery job , that will create a new dataset in our project:","title":"Data analysis and refinery"},{"location":"analyze/ws-ml-dev/#autoai","text":"AutoAI uses data to automatically select the best supervised algorithm to determine the best classification or regression models with optimized hyper parameters. Within a project, we can add an Auto AI Experiment : Then specify a name and server configuration: Add a existing data source (the one prepared by the refinery job), and then specify the column to use for prediction (AnomalyFlag column): Then execute the prediction experiment. AutoAI will do different steps, split the data, prepare data, and then select model or algorithm that may better address the problem. For classification model, it will select among 30 potential candidates: decision tree, random forest, LGBM, XGBoost... Each algorithm selection will generate a pipeline which will be scored to present the most accurate one: The resulting experiments are ranked. When models or pipelines are created, we can see the details of each model, to see how they performed. The confusion matrix for the experiment ranked #1: The tool offers nice capabilities like the feature transformation: And the feature importance ranking, which helps to assess what are the features that impact the classification. Once the model is saved, it can be added to a Catalog, and then being deployed.","title":"AutoAI"},{"location":"analyze/ws-ml-dev/#model-deployment","text":"Once a model is created is promoted to a \"space\". A space contains an overview of deployment status, the deployable assets, associated input and output data, and the associated environments. To access the deployment space, use the main left menu under Analyze -> Analytics deployment spaces . And under a given space we can see the assets and deployments in this space: On the Assets page, you can view the assets in the deployment space (Here we have our AutoAI experimental model deployed). You can see how many deployments each asset has and whether the asset is configured for performance monitoring. The following figure displays the deployed model And going into the deployed model view, we can see the API end point: And even test the prediction from the user interface. A next step is to infuse this model into the scoring application...","title":"Model Deployment"},{"location":"analyze/ws-ml-dev/#notebook","text":"In this section, we are developing a notebook on the telemetries dataset. We have already a project, and defined a data set from the data collection step , so we need to add a notebook. For that, in the project view, use Add to project ... and select the Notebook, specify a name and select python 3.6 runtime... Add a cell to access pandas, and maplablib in the first cell, and then get the code for accessing the dataset, by using the top right button: To Be Continued...","title":"Notebook"},{"location":"bpm/readme/","text":"Maintenance field engineer dispatching business process Problem statement When the anomaly detection scoring service creates a maintenance record on the containers topic, the container microservice will dispatch a field engineer so that the engineer can go the reefer container if it was unloaded in the destination harbor. The process of scheduling an engineer and then completing the work can best be facilitated through a process based, structured workflow. We will be using IBM BPM on Cloud or Cloud Pak for Automation to best demonstrate the workflow. Before creating a ticket or while adding information to the ticket, the process may get the container estimated arrival time and destination harbor to do the dispatching. This means accessing an API on container microservice. Business Process Model This process demonstrates the flow of the task through the process workflow Craft Voyage Payload Script - This step assembles the payload for communicating to the reefer container microservice for status updates. We will craft it from the incoming payload. Set Machine To Maintenance Service Flow - This step creates a REST call to the reefer container to set the machine to Maintenance Mode Schedule Dispatch User Interface - This step gives the scheduler team an interface to look at the given maintenance request and reefer container with issues and manually assign a worker to provide physical support on it Work Order Completion User Interface - This step allows the worker to complete a simple form marking the request as fixed and updating comments relating to the effort Was Completed? Decision Gateway - This decision gateway determines whether to reassign the task to the worker to fill out on a later date (such as equipment not being available), or allows to finish the process Set Machine Off Maintenance Service Flow - This step initiates a REST call to process the changes, capacity, and updated information from the previous captured in the Work Order Completion step. Deploy TWX to BPM on Cloud The following simple video shows you how to deploy the exported 'twx' file to BPM on Cloud. The same can be done on Cloud Pak for automation. Demo of Workflow The following short video presents how to capture our CSRF Token for the Call to Kick off BPM/BAW We get the CSRF Token at first, and then find the Acronym such as via the Designer When we actually execute the request we need the CSRF Token, the Model, the Acronym, and the input data Here we can see the Model as the Process Name directly from Designer Our input data is the input variables in Name,Value pair format We then execute the request and get our 201 Success! In this step we open up process portal, claim, and see our Dispatch Scheduler UI. When we click \"Retrieve Current Location\" we query the container via ID to get updated container info if needed. This code also executes on Load. It is then the Scheduler's job to assign a worker, so we rely on a Worker to be selected from the dropdown We now see the Work Order Completion UI, the worker will pick up the task and work it. We allow for the latitude and logitude to be updated, the capacity to be modified, an audit log to capture comment history, and whether or not the work was able to be completed. Upon finishing we complete the Work Order and via BPM/BAW message the reefer container microservices with the new status Here we see the container updated with the value of 25 we put in as a worker","title":"Engineer dispatching with Business process with Cloud Pak for Automation"},{"location":"bpm/readme/#maintenance-field-engineer-dispatching-business-process","text":"","title":"Maintenance field engineer dispatching business process"},{"location":"bpm/readme/#problem-statement","text":"When the anomaly detection scoring service creates a maintenance record on the containers topic, the container microservice will dispatch a field engineer so that the engineer can go the reefer container if it was unloaded in the destination harbor. The process of scheduling an engineer and then completing the work can best be facilitated through a process based, structured workflow. We will be using IBM BPM on Cloud or Cloud Pak for Automation to best demonstrate the workflow. Before creating a ticket or while adding information to the ticket, the process may get the container estimated arrival time and destination harbor to do the dispatching. This means accessing an API on container microservice.","title":"Problem statement"},{"location":"bpm/readme/#business-process-model","text":"This process demonstrates the flow of the task through the process workflow Craft Voyage Payload Script - This step assembles the payload for communicating to the reefer container microservice for status updates. We will craft it from the incoming payload. Set Machine To Maintenance Service Flow - This step creates a REST call to the reefer container to set the machine to Maintenance Mode Schedule Dispatch User Interface - This step gives the scheduler team an interface to look at the given maintenance request and reefer container with issues and manually assign a worker to provide physical support on it Work Order Completion User Interface - This step allows the worker to complete a simple form marking the request as fixed and updating comments relating to the effort Was Completed? Decision Gateway - This decision gateway determines whether to reassign the task to the worker to fill out on a later date (such as equipment not being available), or allows to finish the process Set Machine Off Maintenance Service Flow - This step initiates a REST call to process the changes, capacity, and updated information from the previous captured in the Work Order Completion step.","title":"Business Process Model"},{"location":"bpm/readme/#deploy-twx-to-bpm-on-cloud","text":"The following simple video shows you how to deploy the exported 'twx' file to BPM on Cloud. The same can be done on Cloud Pak for automation.","title":"Deploy TWX to BPM on Cloud"},{"location":"bpm/readme/#demo-of-workflow","text":"The following short video presents how to capture our CSRF Token for the Call to Kick off BPM/BAW We get the CSRF Token at first, and then find the Acronym such as via the Designer When we actually execute the request we need the CSRF Token, the Model, the Acronym, and the input data Here we can see the Model as the Process Name directly from Designer Our input data is the input variables in Name,Value pair format We then execute the request and get our 201 Success! In this step we open up process portal, claim, and see our Dispatch Scheduler UI. When we click \"Retrieve Current Location\" we query the container via ID to get updated container info if needed. This code also executes on Load. It is then the Scheduler's job to assign a worker, so we rely on a Worker to be selected from the dropdown We now see the Work Order Completion UI, the worker will pick up the task and work it. We allow for the latitude and logitude to be updated, the capacity to be modified, an audit log to capture comment history, and whether or not the work was able to be completed. Upon finishing we complete the Work Order and via BPM/BAW message the reefer container microservices with the new status Here we see the container updated with the value of 25 we put in as a worker","title":"Demo of Workflow"},{"location":"collect/cp4d-collect-data/","text":"IBM Cloud Pak for Data: data collection To develop the anomaly predictive service we first need to access the data. We have two datasources in this example: the product information and the telemetries data coming from the different Reefer Containers. With the telemetries we should be able to assess anomaly. The Telemetries are saved to a noSQL database. We are using MongoDB on IBM Cloud. Using Mongo Compass , we can see one of telemetry document as saved into MongoDB. Figure 1: Mongo DB Compass: ibmcloud.telemetries collection It is important to note that the Json document has sensors document embedded. As we will see later they will be mapped to different tables in Cloud Pak Virtualization. As part of the data governance capability, a user with data engineer role can do the following tasks: Define one to many connections to the remote different data sources Create virtual assets to materialize tables and views from the different data sources Assign an asset to an exisint project or a data request (governance object to ask to access data) Define connection First we need to get the connection information for the MongoDB database. See this note for information about Mongo DB instance on IBM Cloud. Get the information about the data connection. Figure 2: Mongo DB on IBM Cloud connection information Then download the TLS certificate as pem file: ibmcloud login -a https://cloud.ibm.com -u passcode -p <somecode-you-get-from-your-login> # Define your resource group ibmcloud target -g gse-eda ibmcloud cdb deployment-cacert gse-eda-mongodb > certs/mongodbca.pem Back to Cloud pak for Data, an administrator may define connections as a reusable objects by entering the data sources information. The figure below illustrates the connection configuration to the Mongo DB running on IBM Cloud: Figure 3: Define connection in CP4D Add connection in Cloud Pak for Data Virtualization may help automatically group tables without moving data, so we can group different data elements into a single schema. Create a new project Once logged into Cloud Pak for Data, create a new project. A project is a collection of assets the analytics team work on. It can include data assets and Notebooks, RStudio files Models, scripts... From the main page select the project view: Figure 4: Top level navigation menu and then new project, and select analytics : Figure 5: Add project Select an empty project: Figure 6: Select project type Enter basic information about your project Figure 7: Project metadata The result is the access to the main project page: Figure 8: Project main page Now we need to define data assets into our project... Data Virtualization As introduced in this paragraph , we want to use data virtualization to access the historical telemetry records: The data engineer uses the Data virtualization capability to search for existing tables and add the tables he wants in the cart . For that, he uses the Virtualize menu Figure 9: Data Virtualization menu and then selects Mongo DB in the Filters column and may be apply some search on specific database name. Figure 10: Data Virtualization on Mongo DB Once done, he selects the expected tables and then use Add to cart link. It is important to note that we have two tables to match the telemetry json document and the sensors sub json document. The next step is to assign them to a project: Figure 11: Data Virtualization cart and tables Create a joined view We need to join the telemetries and the sensors data into the same table, to flatten the records. In the current Mongo document, there is a 1 to 1 relationship between telemetry and telemetry sensor, so it is easy to flatten the model in one table. In the Data Virtualization, as a data steward, we select My Virtualized data , and then select TELEMETRICS and TELEMETRICS_SENSORS tables, then the Join view . Within this new panel, we create a join key, by dragging the TELEMETRICS_ID and _ID together: Figure 12: Joining tables Once joined, a new view is created: Figure 13: Join view We see, now, those new assets as part of the project (Use Add to project ). The figure below show this new asset in the project: Figure 14: Telemetries asset in the project With some data: Figure 15: Telemetry data Note It is important to note that building those views will create new connection to the database engine that can be accessed by using external tool. Next Next is to start working within a model \u2192 Next \u2192","title":"Collect data with Cloud Pak for Data"},{"location":"collect/cp4d-collect-data/#ibm-cloud-pak-for-data-data-collection","text":"To develop the anomaly predictive service we first need to access the data. We have two datasources in this example: the product information and the telemetries data coming from the different Reefer Containers. With the telemetries we should be able to assess anomaly. The Telemetries are saved to a noSQL database. We are using MongoDB on IBM Cloud. Using Mongo Compass , we can see one of telemetry document as saved into MongoDB. Figure 1: Mongo DB Compass: ibmcloud.telemetries collection It is important to note that the Json document has sensors document embedded. As we will see later they will be mapped to different tables in Cloud Pak Virtualization. As part of the data governance capability, a user with data engineer role can do the following tasks: Define one to many connections to the remote different data sources Create virtual assets to materialize tables and views from the different data sources Assign an asset to an exisint project or a data request (governance object to ask to access data)","title":"IBM Cloud Pak for Data: data collection"},{"location":"collect/cp4d-collect-data/#define-connection","text":"First we need to get the connection information for the MongoDB database. See this note for information about Mongo DB instance on IBM Cloud. Get the information about the data connection. Figure 2: Mongo DB on IBM Cloud connection information Then download the TLS certificate as pem file: ibmcloud login -a https://cloud.ibm.com -u passcode -p <somecode-you-get-from-your-login> # Define your resource group ibmcloud target -g gse-eda ibmcloud cdb deployment-cacert gse-eda-mongodb > certs/mongodbca.pem Back to Cloud pak for Data, an administrator may define connections as a reusable objects by entering the data sources information. The figure below illustrates the connection configuration to the Mongo DB running on IBM Cloud: Figure 3: Define connection in CP4D Add connection in Cloud Pak for Data Virtualization may help automatically group tables without moving data, so we can group different data elements into a single schema.","title":"Define connection"},{"location":"collect/cp4d-collect-data/#create-a-new-project","text":"Once logged into Cloud Pak for Data, create a new project. A project is a collection of assets the analytics team work on. It can include data assets and Notebooks, RStudio files Models, scripts... From the main page select the project view: Figure 4: Top level navigation menu and then new project, and select analytics : Figure 5: Add project Select an empty project: Figure 6: Select project type Enter basic information about your project Figure 7: Project metadata The result is the access to the main project page: Figure 8: Project main page Now we need to define data assets into our project...","title":"Create a new project"},{"location":"collect/cp4d-collect-data/#data-virtualization","text":"As introduced in this paragraph , we want to use data virtualization to access the historical telemetry records: The data engineer uses the Data virtualization capability to search for existing tables and add the tables he wants in the cart . For that, he uses the Virtualize menu Figure 9: Data Virtualization menu and then selects Mongo DB in the Filters column and may be apply some search on specific database name. Figure 10: Data Virtualization on Mongo DB Once done, he selects the expected tables and then use Add to cart link. It is important to note that we have two tables to match the telemetry json document and the sensors sub json document. The next step is to assign them to a project: Figure 11: Data Virtualization cart and tables","title":"Data Virtualization"},{"location":"collect/cp4d-collect-data/#create-a-joined-view","text":"We need to join the telemetries and the sensors data into the same table, to flatten the records. In the current Mongo document, there is a 1 to 1 relationship between telemetry and telemetry sensor, so it is easy to flatten the model in one table. In the Data Virtualization, as a data steward, we select My Virtualized data , and then select TELEMETRICS and TELEMETRICS_SENSORS tables, then the Join view . Within this new panel, we create a join key, by dragging the TELEMETRICS_ID and _ID together: Figure 12: Joining tables Once joined, a new view is created: Figure 13: Join view We see, now, those new assets as part of the project (Use Add to project ). The figure below show this new asset in the project: Figure 14: Telemetries asset in the project With some data: Figure 15: Telemetry data Note It is important to note that building those views will create new connection to the database engine that can be accessed by using external tool.","title":"Create a joined view"},{"location":"collect/cp4d-collect-data/#next","text":"Next is to start working within a model \u2192 Next \u2192","title":"Next"},{"location":"collect/cp4i-es/","text":"Data ingestion and in motion with Cloud Pak for Integration","title":"Data ingestion and in motion with Cloud Pak for Integration"},{"location":"collect/cp4i-es/#data-ingestion-and-in-motion-with-cloud-pak-for-integration","text":"","title":"Data ingestion and in motion with Cloud Pak for Integration"},{"location":"collect/generate-telemetry/","text":"Generate telemetry data in MongoDB We are using the simulator to generate data. In the industry, when developing new manufactured product, the engineers do not have a lot of data so they also use a mix of real sensors with simulators to create fake but realistic data to develop and test their models. The historical data need to represent failure and represent the characteristics of a Reefer container. We have defined some sensors to get interesting correlated or independent features. As of now, our telemetry event structure can be seen in this avro schema . For the machine learning environment we can use a csv file or mongodb database or kafka topic as data source. The data generation environment looks like in the figure below: Figure 1: Data collection Simulation The simulator can run as a standalone tool (1) to create training and test data to be saved in a remote mongodb database or can be used to save to csv file. when it runs to simulate reefer container telemetry generation (2), it creates events to Kafka topic, and a stream application can save telemetry records to MongoDB too. We use MongoDB as a Service on IBM Cloud in our reference implementations. Figure 2: IBM Cloud Database We have provided the following documented methods for populating the Product database: Create and save telemetry data with Kubernetes Job running on remote cluster (RECOMMENDED) Create local telemetry data manually Save local telemetry data to MongoDB Create and save telemetry data with Kubernetes Job running on remote cluster In an effort to keep development systems as clean as possible and speed up deployment of various scenarios, our deployment tasks have been encapsulated in Kubernetes Jobs . These are runnable on any Kubernetes platform, including OpenShift. The predefined job for this task will create the required telemetry data and save it directly to the configured MongoDB database instance. This is the most direct method for telemetry data generation and what is necessary for the \"happy path\" version of the environment deployment. If you have use case needs that require other locations for data to be saved or transmitted, you can either adapt the Job here or follow the other sections of this document. Utilizing Databases for MongoDB on IBM Cloud, the following Kubernetes Secrets are required to be created from the auto-generated Service credentials in the target namespace: mongodb-url (in the format of hostname-a,hostname-b , as the endpoint is a paired replica set) kubectl create secret generic mongodb-url --from-literal = binding = '1a2...domain.cloud:30796,1a2c....cloud:30796' mongodb-user kubectl create secret generic mongodb-user --from-literal = binding = 'ibm_cloud_...' mongodb-pwd kubectl create secret generic mongodb-pwd --from-literal = binding = '335....223' mongodb-ca-pem (this requires use of the Cloud Databases CLI Plug-in for the IBM Cloud CLI) ibmcloud cdb deployment-cacert [ MongoDB on IBM Cloud service instance name ] > mongodb.crt kubectl create secret generic mongodb-ca-pem --from-literal = binding = \" $( cat mongodb.crt ) \" Review /scripts/createMongoTelemetryData.yaml and update lines 32, 34, 36, or 38 with any additional modifications to the generated telemetry data. Additional rows can be added as needed in the same job execution. The following fields are acceptable inputs: stype can the normal , poweroff , o2sensor , and co2sensor . cid can be C01 , C02 , C03 , or C04 . pid can be P01 , P02 , P03 , or P04 . records can be any positive integer (within reason). Create the create-telemetry-data Job from the root of the refarch-reefer-ml repository: kubectl apply -f scripts/createMongoTelemetryData.yaml You can tail the created pod's output to see the progress of the database initialization: kubectl logs -f --selector = job-name = create-telemetry-data Create local telemetry data manually If not done yet, you can use our docker image to get an isolated python environment. For that do the following preparation steps: # From refarch_reefer_ml folder cd docker docker build -t ibmcase/python -f docker-python-tools.yaml . docker images | grep ibmcase # .... results should include ibmcase/python latest a89153c0e14f The dockerfile installed the needed dependencies and use pipenv. Also we preset the PYTHONPATH environment variable to /home to specify where python should find the application specifics modules. Generate data as csv file Start the python environment From the docker image created before, use the provided script as: # From refarch_reefer_ml folder ./startPythonEnv.sh Generate power off metrics When a reefer container loses power, restart and reloose it, it may become an issue. This is the goal of this simulation. The simulator accepts different arguments as specified below: usage reefer_simulator-tool --stype [ poweroff | co2sensor | o2sensor | normal ] --cid [ C01 | C02 | C03 | C04 | C05 ] --product_id [ P01 | P02 | P03 | P04 | P05 ] --records <the number of records to generate> --file <the filename to create> --append | --db The cid is for the container id. As the simulator is taking some data from internal datasource you can use only one of those values: [C01 | C02 | C03 | C04 | C05] product_id is also one of the value [ P01 | P02 | P03 | P04 | P05] , as the simulator will derive the target temperature and humidity level from its internal datasource: ('P01','Carrots',1,4,0.4), ('P02','Banana',2,6,0.6), ('P03','Salad',1,4,0.4), ('P04','Avocado',2,6,0.4), ('P05','Tomato',1,6,0.4); --db is when you want to save the telemetry into mongodb DB. In this case be sure to have set the credentials and URL in the scripts/setenv.sh script (see the scripts/setenv-tmp.sh template file) --file is to specify a csv file to write the data --append is used to append the output of this run to an existing file: It permits to accumulate different simulation in the same dataset. (Re)create a new file. It is an important step to get the column names as first row. python simulator/reefer_simulator_tool.py --stype poweroff --cid C01 --records 1000 --product_id P02 --file telemetries.csv then new records are added by appending to existing file python simulator/reefer_simulator_tool.py --cid C03 --product_id P03 --records 1000 --file telemetries.csv --stype poweroff --append The results looks like: Generating 1000 poweroff metrics Timestamp ID Temperature ( celsius ) Target_Temperature ( celsius ) Power PowerConsumption ContentType O2 CO2 Time_Door_Open Maintenance_Required Defrost_Cycle 1 .000000 2019 -06-30 T15:43 Z 101 3 .416766 4 17 .698034 6 .662044 1 11 1 8 .735273 0 6 1 .001001 2019 -06-30 T15:43 Z 101 4 .973630 4 3 .701072 8 .457314 1 13 3 5 .699655 0 6 1 .002002 2019 -06-30 T15:43 Z 101 1 .299275 4 7 .629094 From the two previous commands you should have 2001 rows (one for the header which will be used in the model creation): wc -l telemetries.csv 2001 telemetries.csv Generate Co2 sensor malfunction in same file In the same way as above the simulator can generate data for Co2 sensor malfunction using the command: python simulator/reefer_simulator_tool.py --cid C03 --product_id P02 --records 1000 --file basedata --stype co2sensor --append Generate O2 sensor malfunction in same file python simulator/reefer_simulator_tool.py --cid C03 --product_id P02 --records 1000 --file basedata --stype o2sensor --append Save local telemetry data to MongoDB MongoDB is a popular document-based database that allows developers to quickly build projects without worrying about schema. Mongo components include: mongod - the core process that runs the actual database mongos - controls routing to the databases in case of sharding config servers (CSRS) - stores the metadata in a sharded environment. We propose to persist telemetry for a long time period. For example we can configure Kafka topic to persist telemetries over a period of 20 days, but have another component to continuously move events as JSON documents inside Mongodb. Using Mongodb as service on IBM Cloud Create the MongoDB service on IBM cloud using default configuration and add a service credentials to get the mongodb.composed url: (something starting as mongodb://ibm_cloud_e154ff52_ed ) username and password. Set those environment variables in scripts/setenv.sh the export MONGO_DB_URL=\"mongodb://ibm_c...\" Get the TLS certificate as pem file: ibmcloud cdb deployment-cacert gse-eda-mongodb > certs/mongodbca.pem Start python environment Use IBMCLOUD if you use mongodb, postgresql and kafka on cloud, or LOCAL for kafka and postgresql running via docker compose. ./startPythonEnv IBMCLOUD root@03721594782f: cd /home If you are using your own environment, to access mongodb we use the pymongo driver ( pip install pymongo ) The code below is a simple example of how to access mongodb. URL = os . getenv ( 'MONGO_DB_URL' ) client = MongoClient ( URL , ssl = True , ssl_ca_certs = '/home/certs/mongodbca.pem' ) db = client [ 'ibmclouddb' ] # insert a record result = db . telemetries . insert_one ( telemetry ) telemetry = db . telemetries . find_one ({ \"_id\" : ObjectId ( result . inserted_id )}) # get all the records telemetries = db . telemetries . find () for t in telemetries : See the rest of the code in ml/data/ToMongo.py to load records from CSV file, or the simulator/infrastructure/ReeferRepository.py for the one generating metrics and uploading them directly to MongoDB. We propose two approaches to load data to MongoDB: use created csv file, or run the simulator tool connected to MongoDB. Add data from csv file Using the ToMongo.py script we can load the data from the ml/data/telemetries.csv file to mongodb. In a Terminal window uses the following commmand: ./startPythonEnv.sh IBMCLOUD cd ml/data python ToMongo.py Add data using the telemetry repository of the simulator Verify your MONGO* environment variables are set according to your created service in the scriptssetenv.sh file. ./startPythonEnv.sh IBMCLOUD cd simulation python simulator/reefer_simulator_tool.py --cid C03 --product_id P02 --records 1000 --stype poweroff --db python simulator/reefer_simulator_tool.py --cid C03 --product_id P02 --records 1000 --stype co2sensor --db python simulator/reefer_simulator_tool.py --cid C03 --product_id P02 --records 1000 --stype o2sensor --db Verify data with mongo CLI To verify the data loaded into the database we use mongo CLI with the following command: # to connect mongo -u $USERNAME -p $PASSWORD --tls --tlsCAFile mongodb.pem --authenticationDatabase admin --host replset/1a2ce8ca-<>.bn<>c0.databases.appdomain.cloud:30796 --tlsAllowInvalidCertificates The full host name is masked . The USERNAME and PASSWORD are environment variables you set from the IBM Cloud service credentials. Something like: { \"connection\": { \"cli\": { \"arguments\": [ [ \"-u\", \"ibm_cloud_<>_48b1_b899\", \"-p\", \"3359192a<>e5fbdf51fd573676e58aff4f9e223\", The mongodb.pem is the certificate in text save in a file. To get this certificate you can use the following command: ibmcloud cdb deployment-cacert gse-eda-mongodb > mongodb.pem or by using the IBM Cloud mongodb service user interface (Manage menu): Once connected use the ibmclouddb database with mongo CLI: MongoDB server version: 4.2.0 replset:PRIMARY> use ibmclouddb replset:PRIMARY> db.getCollection(\"telemetries\").find() Using MongoDB on Openshift 4.2 on-premise We assume you have provisioned an Openshift 4.2 cluster and logged in. We use the following container image: centos/mongodb-36-centos7 . So to install it, we use the following command: oc new-app -e \\ MONGODB_USER=mongo,MONGODB_PASSWORD=<password>,MONGODB_DATABASE=reeferdb,MONGODB_ADMIN_PASSWORD=<admin_password> \\ centos/mongodb-36-centos7 Connect to the pod and then use the mongo CLI $ oc get pods NAME READY STATUS RESTARTS AGE mongodb-36-centos7-1-wcn7h 1 /1 Running 0 4d $ oc rsh mongodb-36-centos7-1-wcn7h bash-4.2$ mongo -u $MONGODB_USER -p $MONGODB_PASSWORD $MONGODB_DATABASE MongoDB shell version: 2 .4.9 connecting to: reeferdb > show collections To remove the db on openshift: oc delete dc The document saved in mongo will not be a flat record as produced by the simulator when it creates records to Kafka, but a Json document which matches the following format, where sensors are in their own object: { \"timestamp\" : \"2019-09-04 T15:31 Z\" , \"containerID\" : \"C100\" , \"product_id\" : \"P02\" , \"content_type\" : 2 , \"sensors\" : { \"temperature\" : 5.49647 , \"oxygen_level\" : 20.4543 , \"nitrogen_level\" : 79.4046 , \"carbon_dioxide_level\" : 4.42579 , \"humidity_level\" : 60.3148 , \"fan_1\" : \"True\" , \"fan_2\" : \"True\" , \"fan_3\" : \"True\" , \"ambiant_temperature\" : 19.8447 }, \"target_temperature\" : 6.0 , \"kilowatts\" : 3.44686 , \"latitude\" : \"37.8226902168957,\" , \"longitude\" : \"-122.3248956640928\" , \"time_door_open\" : 0 , \"defrost_cycle\" : 6 } First be sure to set at least the following environment variables in the setenv.sh file MONGO_DB_URL, MONGO_SSL_PEM If not done before or to ensure the mongo connection works fine, run the ReeferRepository.py tool to create the telemetries collection using the following command: ./startPythonEnv.sh IBMCLOUD > python simulator/infrastructure/ReeferRepository.py Delete records in database In mongo CLI do: db.telemetries.deleteMany ({})","title":"Generate telemetry data"},{"location":"collect/generate-telemetry/#generate-telemetry-data-in-mongodb","text":"We are using the simulator to generate data. In the industry, when developing new manufactured product, the engineers do not have a lot of data so they also use a mix of real sensors with simulators to create fake but realistic data to develop and test their models. The historical data need to represent failure and represent the characteristics of a Reefer container. We have defined some sensors to get interesting correlated or independent features. As of now, our telemetry event structure can be seen in this avro schema . For the machine learning environment we can use a csv file or mongodb database or kafka topic as data source. The data generation environment looks like in the figure below: Figure 1: Data collection Simulation The simulator can run as a standalone tool (1) to create training and test data to be saved in a remote mongodb database or can be used to save to csv file. when it runs to simulate reefer container telemetry generation (2), it creates events to Kafka topic, and a stream application can save telemetry records to MongoDB too. We use MongoDB as a Service on IBM Cloud in our reference implementations. Figure 2: IBM Cloud Database We have provided the following documented methods for populating the Product database: Create and save telemetry data with Kubernetes Job running on remote cluster (RECOMMENDED) Create local telemetry data manually Save local telemetry data to MongoDB","title":"Generate telemetry data in MongoDB"},{"location":"collect/generate-telemetry/#create-and-save-telemetry-data-with-kubernetes-job-running-on-remote-cluster","text":"In an effort to keep development systems as clean as possible and speed up deployment of various scenarios, our deployment tasks have been encapsulated in Kubernetes Jobs . These are runnable on any Kubernetes platform, including OpenShift. The predefined job for this task will create the required telemetry data and save it directly to the configured MongoDB database instance. This is the most direct method for telemetry data generation and what is necessary for the \"happy path\" version of the environment deployment. If you have use case needs that require other locations for data to be saved or transmitted, you can either adapt the Job here or follow the other sections of this document. Utilizing Databases for MongoDB on IBM Cloud, the following Kubernetes Secrets are required to be created from the auto-generated Service credentials in the target namespace: mongodb-url (in the format of hostname-a,hostname-b , as the endpoint is a paired replica set) kubectl create secret generic mongodb-url --from-literal = binding = '1a2...domain.cloud:30796,1a2c....cloud:30796' mongodb-user kubectl create secret generic mongodb-user --from-literal = binding = 'ibm_cloud_...' mongodb-pwd kubectl create secret generic mongodb-pwd --from-literal = binding = '335....223' mongodb-ca-pem (this requires use of the Cloud Databases CLI Plug-in for the IBM Cloud CLI) ibmcloud cdb deployment-cacert [ MongoDB on IBM Cloud service instance name ] > mongodb.crt kubectl create secret generic mongodb-ca-pem --from-literal = binding = \" $( cat mongodb.crt ) \" Review /scripts/createMongoTelemetryData.yaml and update lines 32, 34, 36, or 38 with any additional modifications to the generated telemetry data. Additional rows can be added as needed in the same job execution. The following fields are acceptable inputs: stype can the normal , poweroff , o2sensor , and co2sensor . cid can be C01 , C02 , C03 , or C04 . pid can be P01 , P02 , P03 , or P04 . records can be any positive integer (within reason). Create the create-telemetry-data Job from the root of the refarch-reefer-ml repository: kubectl apply -f scripts/createMongoTelemetryData.yaml You can tail the created pod's output to see the progress of the database initialization: kubectl logs -f --selector = job-name = create-telemetry-data","title":"Create and save telemetry data with Kubernetes Job running on remote cluster"},{"location":"collect/generate-telemetry/#create-local-telemetry-data-manually","text":"If not done yet, you can use our docker image to get an isolated python environment. For that do the following preparation steps: # From refarch_reefer_ml folder cd docker docker build -t ibmcase/python -f docker-python-tools.yaml . docker images | grep ibmcase # .... results should include ibmcase/python latest a89153c0e14f The dockerfile installed the needed dependencies and use pipenv. Also we preset the PYTHONPATH environment variable to /home to specify where python should find the application specifics modules.","title":"Create local telemetry data manually"},{"location":"collect/generate-telemetry/#generate-data-as-csv-file","text":"","title":"Generate data as csv file"},{"location":"collect/generate-telemetry/#start-the-python-environment","text":"From the docker image created before, use the provided script as: # From refarch_reefer_ml folder ./startPythonEnv.sh","title":"Start the python environment"},{"location":"collect/generate-telemetry/#generate-power-off-metrics","text":"When a reefer container loses power, restart and reloose it, it may become an issue. This is the goal of this simulation. The simulator accepts different arguments as specified below: usage reefer_simulator-tool --stype [ poweroff | co2sensor | o2sensor | normal ] --cid [ C01 | C02 | C03 | C04 | C05 ] --product_id [ P01 | P02 | P03 | P04 | P05 ] --records <the number of records to generate> --file <the filename to create> --append | --db The cid is for the container id. As the simulator is taking some data from internal datasource you can use only one of those values: [C01 | C02 | C03 | C04 | C05] product_id is also one of the value [ P01 | P02 | P03 | P04 | P05] , as the simulator will derive the target temperature and humidity level from its internal datasource: ('P01','Carrots',1,4,0.4), ('P02','Banana',2,6,0.6), ('P03','Salad',1,4,0.4), ('P04','Avocado',2,6,0.4), ('P05','Tomato',1,6,0.4); --db is when you want to save the telemetry into mongodb DB. In this case be sure to have set the credentials and URL in the scripts/setenv.sh script (see the scripts/setenv-tmp.sh template file) --file is to specify a csv file to write the data --append is used to append the output of this run to an existing file: It permits to accumulate different simulation in the same dataset. (Re)create a new file. It is an important step to get the column names as first row. python simulator/reefer_simulator_tool.py --stype poweroff --cid C01 --records 1000 --product_id P02 --file telemetries.csv then new records are added by appending to existing file python simulator/reefer_simulator_tool.py --cid C03 --product_id P03 --records 1000 --file telemetries.csv --stype poweroff --append The results looks like: Generating 1000 poweroff metrics Timestamp ID Temperature ( celsius ) Target_Temperature ( celsius ) Power PowerConsumption ContentType O2 CO2 Time_Door_Open Maintenance_Required Defrost_Cycle 1 .000000 2019 -06-30 T15:43 Z 101 3 .416766 4 17 .698034 6 .662044 1 11 1 8 .735273 0 6 1 .001001 2019 -06-30 T15:43 Z 101 4 .973630 4 3 .701072 8 .457314 1 13 3 5 .699655 0 6 1 .002002 2019 -06-30 T15:43 Z 101 1 .299275 4 7 .629094 From the two previous commands you should have 2001 rows (one for the header which will be used in the model creation): wc -l telemetries.csv 2001 telemetries.csv","title":"Generate power off metrics"},{"location":"collect/generate-telemetry/#generate-co2-sensor-malfunction-in-same-file","text":"In the same way as above the simulator can generate data for Co2 sensor malfunction using the command: python simulator/reefer_simulator_tool.py --cid C03 --product_id P02 --records 1000 --file basedata --stype co2sensor --append","title":"Generate Co2 sensor malfunction in same file"},{"location":"collect/generate-telemetry/#generate-o2-sensor-malfunction-in-same-file","text":"python simulator/reefer_simulator_tool.py --cid C03 --product_id P02 --records 1000 --file basedata --stype o2sensor --append","title":"Generate O2 sensor malfunction in same file"},{"location":"collect/generate-telemetry/#save-local-telemetry-data-to-mongodb","text":"MongoDB is a popular document-based database that allows developers to quickly build projects without worrying about schema. Mongo components include: mongod - the core process that runs the actual database mongos - controls routing to the databases in case of sharding config servers (CSRS) - stores the metadata in a sharded environment. We propose to persist telemetry for a long time period. For example we can configure Kafka topic to persist telemetries over a period of 20 days, but have another component to continuously move events as JSON documents inside Mongodb.","title":"Save local telemetry data to MongoDB"},{"location":"collect/generate-telemetry/#using-mongodb-as-service-on-ibm-cloud","text":"Create the MongoDB service on IBM cloud using default configuration and add a service credentials to get the mongodb.composed url: (something starting as mongodb://ibm_cloud_e154ff52_ed ) username and password. Set those environment variables in scripts/setenv.sh the export MONGO_DB_URL=\"mongodb://ibm_c...\" Get the TLS certificate as pem file: ibmcloud cdb deployment-cacert gse-eda-mongodb > certs/mongodbca.pem","title":"Using Mongodb as service on IBM Cloud"},{"location":"collect/generate-telemetry/#start-python-environment","text":"Use IBMCLOUD if you use mongodb, postgresql and kafka on cloud, or LOCAL for kafka and postgresql running via docker compose. ./startPythonEnv IBMCLOUD root@03721594782f: cd /home If you are using your own environment, to access mongodb we use the pymongo driver ( pip install pymongo ) The code below is a simple example of how to access mongodb. URL = os . getenv ( 'MONGO_DB_URL' ) client = MongoClient ( URL , ssl = True , ssl_ca_certs = '/home/certs/mongodbca.pem' ) db = client [ 'ibmclouddb' ] # insert a record result = db . telemetries . insert_one ( telemetry ) telemetry = db . telemetries . find_one ({ \"_id\" : ObjectId ( result . inserted_id )}) # get all the records telemetries = db . telemetries . find () for t in telemetries : See the rest of the code in ml/data/ToMongo.py to load records from CSV file, or the simulator/infrastructure/ReeferRepository.py for the one generating metrics and uploading them directly to MongoDB. We propose two approaches to load data to MongoDB: use created csv file, or run the simulator tool connected to MongoDB.","title":"Start python environment"},{"location":"collect/generate-telemetry/#add-data-from-csv-file","text":"Using the ToMongo.py script we can load the data from the ml/data/telemetries.csv file to mongodb. In a Terminal window uses the following commmand: ./startPythonEnv.sh IBMCLOUD cd ml/data python ToMongo.py","title":"Add data from csv file"},{"location":"collect/generate-telemetry/#add-data-using-the-telemetry-repository-of-the-simulator","text":"Verify your MONGO* environment variables are set according to your created service in the scriptssetenv.sh file. ./startPythonEnv.sh IBMCLOUD cd simulation python simulator/reefer_simulator_tool.py --cid C03 --product_id P02 --records 1000 --stype poweroff --db python simulator/reefer_simulator_tool.py --cid C03 --product_id P02 --records 1000 --stype co2sensor --db python simulator/reefer_simulator_tool.py --cid C03 --product_id P02 --records 1000 --stype o2sensor --db","title":"Add data using the telemetry repository of the simulator"},{"location":"collect/generate-telemetry/#verify-data-with-mongo-cli","text":"To verify the data loaded into the database we use mongo CLI with the following command: # to connect mongo -u $USERNAME -p $PASSWORD --tls --tlsCAFile mongodb.pem --authenticationDatabase admin --host replset/1a2ce8ca-<>.bn<>c0.databases.appdomain.cloud:30796 --tlsAllowInvalidCertificates The full host name is masked . The USERNAME and PASSWORD are environment variables you set from the IBM Cloud service credentials. Something like: { \"connection\": { \"cli\": { \"arguments\": [ [ \"-u\", \"ibm_cloud_<>_48b1_b899\", \"-p\", \"3359192a<>e5fbdf51fd573676e58aff4f9e223\", The mongodb.pem is the certificate in text save in a file. To get this certificate you can use the following command: ibmcloud cdb deployment-cacert gse-eda-mongodb > mongodb.pem or by using the IBM Cloud mongodb service user interface (Manage menu): Once connected use the ibmclouddb database with mongo CLI: MongoDB server version: 4.2.0 replset:PRIMARY> use ibmclouddb replset:PRIMARY> db.getCollection(\"telemetries\").find()","title":"Verify data with mongo CLI"},{"location":"collect/generate-telemetry/#using-mongodb-on-openshift-42-on-premise","text":"We assume you have provisioned an Openshift 4.2 cluster and logged in. We use the following container image: centos/mongodb-36-centos7 . So to install it, we use the following command: oc new-app -e \\ MONGODB_USER=mongo,MONGODB_PASSWORD=<password>,MONGODB_DATABASE=reeferdb,MONGODB_ADMIN_PASSWORD=<admin_password> \\ centos/mongodb-36-centos7 Connect to the pod and then use the mongo CLI $ oc get pods NAME READY STATUS RESTARTS AGE mongodb-36-centos7-1-wcn7h 1 /1 Running 0 4d $ oc rsh mongodb-36-centos7-1-wcn7h bash-4.2$ mongo -u $MONGODB_USER -p $MONGODB_PASSWORD $MONGODB_DATABASE MongoDB shell version: 2 .4.9 connecting to: reeferdb > show collections To remove the db on openshift: oc delete dc The document saved in mongo will not be a flat record as produced by the simulator when it creates records to Kafka, but a Json document which matches the following format, where sensors are in their own object: { \"timestamp\" : \"2019-09-04 T15:31 Z\" , \"containerID\" : \"C100\" , \"product_id\" : \"P02\" , \"content_type\" : 2 , \"sensors\" : { \"temperature\" : 5.49647 , \"oxygen_level\" : 20.4543 , \"nitrogen_level\" : 79.4046 , \"carbon_dioxide_level\" : 4.42579 , \"humidity_level\" : 60.3148 , \"fan_1\" : \"True\" , \"fan_2\" : \"True\" , \"fan_3\" : \"True\" , \"ambiant_temperature\" : 19.8447 }, \"target_temperature\" : 6.0 , \"kilowatts\" : 3.44686 , \"latitude\" : \"37.8226902168957,\" , \"longitude\" : \"-122.3248956640928\" , \"time_door_open\" : 0 , \"defrost_cycle\" : 6 } First be sure to set at least the following environment variables in the setenv.sh file MONGO_DB_URL, MONGO_SSL_PEM If not done before or to ensure the mongo connection works fine, run the ReeferRepository.py tool to create the telemetries collection using the following command: ./startPythonEnv.sh IBMCLOUD > python simulator/infrastructure/ReeferRepository.py","title":"Using MongoDB on Openshift 4.2 on-premise"},{"location":"collect/generate-telemetry/#delete-records-in-database","text":"In mongo CLI do: db.telemetries.deleteMany ({})","title":"Delete records in database"},{"location":"collect/products-postgres/","text":"Define the products data into postgresql The Simulator references product data stored in a Postgresql database. There are multiple ways to populate this database depending on your level of experience with Postgresql, database services, and your local development environment. We have provided the following documented methods for populating the Product database: Kubernetes Job running on remote cluster (RECOMMENDED) Docker image running on local machine Postgresql CLI (psql) running on local machine Kubernetes Job running on remote cluster In an effort to keep development systems as clean as possible and speed up deployment of various scenarios, our deployment tasks have been encapsulated in Kubernetes Jobs . These are runnable on any Kubernetes platform, including OpenShift. Following the configuration prerequisistes defined in the Backing Services documentation for using Databases for PostgreSQL on IBM Cloud, you should already have the following Kubernetes Secrets defined in your target namespace: postgresql-url (in the format of jdbc:postgresql://<hostname>:<port>/<database-name>?sslmode=... ) kubectl create secret generic postgresql-url --from-literal = binding = 'jdbc:postgresql://<hostname>:<port>/<database-name>?sslmode=...' postgresql-user kubectl create secret generic postgresql-user --from-literal = binding = 'ibm_cloud_...' postgresql-pwd kubectl create secret generic postgresql-pwd --from-literal = binding = '1a2...9z0' postgresql-ca-pem (this requires use of the Cloud Databases CLI Plug-in for the IBM Cloud CLI) ibmcloud cdb deployment-cacert [ PostgreSQL on IBM Cloud service instance name ] > postgres.crt kubectl create secret generic postgresql-ca-pem --from-literal = binding = \" $( cat postgres.crt ) \" Create the create-postgres-tables Job from the root of the refarch-reefer-ml repository: kubectl apply -f scripts/createPGtables.yaml You can tail the created pod's output to see the progress of the database initialization: kubectl logs -f --selector = job-name = create-postgres-tables Docker image running on local machine The simulator code includes the infrastructure/ProductRepository.py that creates tables and adds some product definitions inside the table. Uncomment line 101 from /simulator/infrastructure/ProductRepository.py : # repo.populateProductsReferenceData() The following command is using our python environment docker image and the python code: ./scripts/createPGTables.sh IBMCLOUD Postgresql CLI (psql) running on local machine An alternate techniques is to use psql as described in this section. Previous experience with PSQL is recommended. We use a docker image to run psql: $ cd scripts $ ./startPsql.sh IBMCLOUD $ PGPASSWORD = $POSTGRES_PWD psql --host = $HOST --port = $PORT --username = $POSTGRES_USER --dbname = $POSTGRES_DB ibmclouddb = > List relations... ibmclouddb => \\d Then create table if not done before: ibmclouddb => CREATE TABLE products ( product_id varchar ( 64 ) NOT NULL PRIMARY KEY , description varchar ( 100 ), target_temperature REAL , target_humidity_level REAL ); Populate the data: ibmclouddb => INSERT INTO products ( product_id , description , target_temperature , target_humidity_level ) VALUES ( 'P01' , 'Carrots' , 4 , 0.4 ), ( 'P02' , 'Banana' , 6 , 0.6 ), ( 'P03' , 'Salad' , 4 , 0.4 ), ( 'P04' , 'Avocado' , 6 , 0.4 ), ( 'P05' , 'Tomato' , 4 , 0.4 ); List the products SELECT * FROM products ; You should see: product_id | description | target_temperature | target_humidity_level | content_type ------------+-------------+--------------------+-----------------------+-------------- P01 | Carrots | 4 | 0.4 | 1 P02 | Banana | 6 | 0.6 | 2 P03 | Salad | 4 | 0.4 | 1 P04 | Avocado | 6 | 0.4 | 2 P05 | Tomato | 6 | 0.3 | 6 Exit the PSQL environment ibmclouddb => \\q","title":"Define the products data into postgresql"},{"location":"collect/products-postgres/#define-the-products-data-into-postgresql","text":"The Simulator references product data stored in a Postgresql database. There are multiple ways to populate this database depending on your level of experience with Postgresql, database services, and your local development environment. We have provided the following documented methods for populating the Product database: Kubernetes Job running on remote cluster (RECOMMENDED) Docker image running on local machine Postgresql CLI (psql) running on local machine","title":"Define the products data into postgresql"},{"location":"collect/products-postgres/#kubernetes-job-running-on-remote-cluster","text":"In an effort to keep development systems as clean as possible and speed up deployment of various scenarios, our deployment tasks have been encapsulated in Kubernetes Jobs . These are runnable on any Kubernetes platform, including OpenShift. Following the configuration prerequisistes defined in the Backing Services documentation for using Databases for PostgreSQL on IBM Cloud, you should already have the following Kubernetes Secrets defined in your target namespace: postgresql-url (in the format of jdbc:postgresql://<hostname>:<port>/<database-name>?sslmode=... ) kubectl create secret generic postgresql-url --from-literal = binding = 'jdbc:postgresql://<hostname>:<port>/<database-name>?sslmode=...' postgresql-user kubectl create secret generic postgresql-user --from-literal = binding = 'ibm_cloud_...' postgresql-pwd kubectl create secret generic postgresql-pwd --from-literal = binding = '1a2...9z0' postgresql-ca-pem (this requires use of the Cloud Databases CLI Plug-in for the IBM Cloud CLI) ibmcloud cdb deployment-cacert [ PostgreSQL on IBM Cloud service instance name ] > postgres.crt kubectl create secret generic postgresql-ca-pem --from-literal = binding = \" $( cat postgres.crt ) \" Create the create-postgres-tables Job from the root of the refarch-reefer-ml repository: kubectl apply -f scripts/createPGtables.yaml You can tail the created pod's output to see the progress of the database initialization: kubectl logs -f --selector = job-name = create-postgres-tables","title":"Kubernetes Job running on remote cluster"},{"location":"collect/products-postgres/#docker-image-running-on-local-machine","text":"The simulator code includes the infrastructure/ProductRepository.py that creates tables and adds some product definitions inside the table. Uncomment line 101 from /simulator/infrastructure/ProductRepository.py : # repo.populateProductsReferenceData() The following command is using our python environment docker image and the python code: ./scripts/createPGTables.sh IBMCLOUD","title":"Docker image running on local machine"},{"location":"collect/products-postgres/#postgresql-cli-psql-running-on-local-machine","text":"An alternate techniques is to use psql as described in this section. Previous experience with PSQL is recommended. We use a docker image to run psql: $ cd scripts $ ./startPsql.sh IBMCLOUD $ PGPASSWORD = $POSTGRES_PWD psql --host = $HOST --port = $PORT --username = $POSTGRES_USER --dbname = $POSTGRES_DB ibmclouddb = > List relations... ibmclouddb => \\d Then create table if not done before: ibmclouddb => CREATE TABLE products ( product_id varchar ( 64 ) NOT NULL PRIMARY KEY , description varchar ( 100 ), target_temperature REAL , target_humidity_level REAL ); Populate the data: ibmclouddb => INSERT INTO products ( product_id , description , target_temperature , target_humidity_level ) VALUES ( 'P01' , 'Carrots' , 4 , 0.4 ), ( 'P02' , 'Banana' , 6 , 0.6 ), ( 'P03' , 'Salad' , 4 , 0.4 ), ( 'P04' , 'Avocado' , 6 , 0.4 ), ( 'P05' , 'Tomato' , 4 , 0.4 ); List the products SELECT * FROM products ; You should see: product_id | description | target_temperature | target_humidity_level | content_type ------------+-------------+--------------------+-----------------------+-------------- P01 | Carrots | 4 | 0.4 | 1 P02 | Banana | 6 | 0.6 | 2 P03 | Salad | 4 | 0.4 | 1 P04 | Avocado | 6 | 0.4 | 2 P05 | Tomato | 6 | 0.3 | 6 Exit the PSQL environment ibmclouddb => \\q","title":"Postgresql CLI (psql) running on local machine"},{"location":"devops/cd/","text":"Continuous Deployment Our Continuous Deployment (CD) approach focuses on a GitOps-based deployment model, using Git as a single source of truth for the deployment, management, and operations of our running application components. In this model, we have the flexibility to use multiple open-source to apply the single source of truth from a given Git repository onto a desired cluster environment. More detail around the background of GitOps and how it differs from traditional deployment models can be found in this blog post from WeaveWorks. One of the main tools that we use in this space is a GitOps-focused continuous deployment project named ArgoCD . As documented by the IBM Garage for Cloud team, ArgoCD can monitor GitHub-based projects and apply changes stored in that repository's YAML files to a running Kubernetes-based cluster. We have documented our general ArgoCD Continuous Delivery workflow in the ibm-cloud-architecture/refarch-kc-gitops repository. The details of our ArgoCD-based GitOps deployments are covered in the ArgoCD deployments section below. Another DevOps tool which provides the opportunity to deployment applications via the GitOps methodology is Tekton . The Tekton Pipelines project provides a declarative language for defining and executing both CI and CD-style pipelines, all defined with common Kubernetes-like nomenclature. It even has the capability to kick off pipeline runs based off of GitHub webhooks. The details of our Tekton & Appsody deployments are covered in the Tekton & Appsody deployments section below. ArgoCD deployments Our main continuous deployment pattern operates on the same principle of \"zero-infrastructure overhead\" as our continuous integration implementations. This allows us to be agile, adaptable, and efficient in what we deploy where. ArgoCD is a perfect companion to this principle, as we do not need additional long-running CD infrastructure to monitor either a source environment or a target deployment environment. Our CI process sits with our code (on the same hosted infrastructure), while our CD process sits with the target deployment environment (on the same Kubernetes-based cluster). To utilize ArgoCD in this manner, we define a set of Kubernetes YAMLs generated from helm template commands, with environment, namespace, and cluster-specific parameters provided as needed. The details of generating those helm template YAMLs can be found in our main Application Components documentation. The templated YAMLs are generated with the names of the required ConfigMaps and Secrets specific to a namespace on the eventual target deployment cluster. This allows us to create a deployment artifact programmatically without exposing confidential and secret credentials via source code. Once these YAMLs are generated, they are checked in to the main GitOps repository for the project, under a new branch with a branch name in the format of <namespace>/<cluster> for ease of identification. These will then have a folder structure of /<component-name>/templates/<artifact-type>.yaml , with most components providing Deployment, Service, and Route artifact YAMLs. An ArgoCD application is then created on the ArgoCD deployment inside the target environment that can read from the GitOps repository. ArgoCD can also deploy between clusters, which does come in handy in certain use cases, but remember our squads main goal of \"zero-infrastructure overhead\" , so we deploy from ArgoCD into the same cluster it is deployed on the majority of the time. The ArgoCD application is a Custom-Resource Definition, comprising of the details necessary to determine the remote code repository URL, the branch of the code to use, the target namespace, and any formatting capabilities that are necessary. ArgoCD then handles automatically (or manually) syncing the deployments in the target namespace with the state that is described in the YAMLs on the specific branch in the GitOps repository. To keep in sync with the continuous integration implementation we have defined in Continuous integration , we have an additional GitHub Actions workflow defined in this repository that will update the YAML files contained in the repository with the latest microservice container images as they are modified, thus enabling a completely automated build-to-deployment lifecycle. Defined in the .github/workflows/update-gitops-deployments-(eda-integration).yaml workflow file, the workflow will scan the repository for all templated use of container images in Kubernetes Deployment files (and recently updated to be extensible to any YAML-based file!), search Docker Hub for the latest version of that container image, update the YAML file in-place, and check in the YAML updates back to the same repository and branch. This process is kicked off by the webhook Jobs mentioned in our CI process, as well as on a regularly-scheduled cron-like timer. Deploying a microservice with ArgoCD You can deploy both the simulator and the scoring-mp microservices using ArgoCD in this reference implementation. The process is the same for either microservice, with minor parameter configuration being the only difference between the two. Prerequisites Ensure all necessary Kubernetes ConfigMaps and Secrets have been created in the namespace in which the application will be running. Ensure ArgoCD has been deployed to the local cluster with connectivity to the internet. Using the ArgoCD dashboard Access the ArgoCD Dashboard via it's exposed Route in the argocd namespace. This should be in the form of https://argocd-server-argocd.apps.green.ocp.csplab.local . Depending upon your cluster and ArgoCD, you will have specific login credentials. Login as directed and click NEW APPLICATION . Enter the following parameters for either the SpringContainerMS or scoring-mp microservice and click CREATE . SpringContainerMS Application Name: springcontainerms Project: Select default from the drop-down Sync Policy: Select Automatic from the drop-down Repository URL: https://github.com/ibm-cloud-architecture/refarch-kc-gitops.git Revision: eda-sandbox/api-green-ocp-csplab-local (this is the branch in the GitOps repository for our specific project on our specific cluster) Path: springcontainerms Cluster: https://kubernetes.default.svc (this is our local cluster) Namespace: Your desired target namespace Check the Directory: Include subdirectories checkbox scoring-mp Application Name: scoring-mp Project: Select default from the drop-down Sync Policy: Select Automatic from the drop-down Repository URL: https://github.com/ibm-cloud-architecture/refarch-kc-gitops.git Revision: eda-sandbox/api-green-ocp-csplab-local (this is the branch in the GitOps repository for our specific project on our specific cluster) Path: scoring-mp Cluster: https://kubernetes.default.svc (this is our local cluster) Namespace: Your desired target namespace Check the Directory: Include subdirectories checkbox Once the application is successfully created inside ArgoCD, you can click the application tile to see the latest status of the ArgoCD-managed, GitOps-deployed microservice instance. It should begin synchronizing immediately upon creation. As ArgoCD applies the desired configuration to the cluster, you should see the pods of the microservice being created: kubectl get Pods Using the command-line interface ArgoCD provides a command-line interface as well, however we are not covering that in this reference implementation. Once you are satisfied with the ArgoCD dashboard-based deployment pattern, you can reference the ArgoCD Getting Started docs for further details on using the ArgoCD CLI and CRD YAMLs for programmatic interaction. Tekton & Appsody deployments We have also implemented some facets of the project deployment workflows using the Tekton Pipelines project and its inherent ease of support of the Appsody open-source developer experience project through the standard integration between the two built into the Kabanero open-source project, or more formally, the IBM Cloud Pak for Applications . Defined in the /scripts/tekton directory, we have a simple pipeline that will utilize the appsody deploy command to deploy the generated AppsodyApplication CRD YAML to the target environment. Similar to our ArgoCD-based deployments of Helm-generated, standard Kubernetes YAMLs, AppsodyApplication YAMLs can also be deployed through ArgoCD in a GitOps manner. However, for demonstration inside this project, additional capabilities are provided to showcase how we can utilize different pieces of the platform to deploy similar applications when different requirements are presented. Similar to ArgoCD, Tekton Pipelines run on the same cluster (and often in the same namespace!) as your running application code, thus allowing for more programmatic control over the deployment, management, operations, and existence of your application components. The key artifact that enables Tekton to deploy our Appsody-based refarch-reefer-ml/simulator microservice is the generated app-deploy.yaml file. The refarch-reefer-ml/simulator/app-deploy.yaml file was generated according to the appsody build command and then annotated with the required environment variables and metadata for successful operation in a given namespace, very similar to the pattern required for generating our Helm-templated YAMLs in the ArgoCD deployments section above. We then make use of the Appsody Operator to apply the AppsodyApplication to the target environment through the appsody deploy --no-build command. As documented in the Appsody Docs , we are able to take advantage of the pre-built container images available on Docker Hub and the annotated app-deploy.yaml file that is now a synonymous GitOps-like deployment artifact to quickly apply the change to the target namespace in the same cluster. Once the appsody deploy command is succesful, the Appsody Operator and Kubernetes takes care of the rest and reconciles the necessary underlying Kubernetes artifacts that are required to fulfill the requirements of serving up the application code in real-time! Deploying the simulator microservice with Tekton & Appsody Prerequisites Ensure all necessary Kubernetes ConfigMaps and Secrets have been created in the namespace in which the application will be running. Ensure an Appsody Operator has been configured to watch the namespace in which the application will be running. Create a new or modify an existing Service Account in the target namespace and bind the required API RBAC requirements for the Appsody Operator. Further details available in the Appsody Docs A sample YAML document has been provided via rbac-sa.yaml , with environment-specific updates to the Namespace and Service Account name fields being required. From the refarch-reefer-ml directory, import the Tekton pipeline artifacts: kubectl apply -f scripts/tekton/ Validate these items have been imported successfully by querying the cluster: kubectl get Pipeline,Task,PipelineResource Using the command-line interface Open the /scripts/tekton/manual/simulator-pipeline-run.yaml file in a text editor and ensure everything makes sense. Kick off a new pipeline run with the Kubernetes CLI: kubectl create -f scripts/tekton/manual/simulator-pipeline-run.yaml You can monitor the pipeline by common kubectl get and kubectl describe commands: kubectl get PipelineRun For further details on how to access the logs of a PipelineRun, reference the Tekton Pipelines documentation . Once the pipeline completes, you should see a deployed instance of the Simulator Appsody application: kubectl get Pods Using the Tekton dashboard Access the Tekton Pipelines dashboard from your Kabanero installation or directly via the defined Route. This will be something similar to https://tekton-dashboard-tekton-pipelines.apps.green.ocp.csplab.local . Click PipelineRuns in the left-nav menu and click Create PipelineRun from the upper-right of the page. Enter the following parameters and click Create : Namespace: Target namespace for application deployment target Pipeline: appsody-deploy-pipeline PipelineResources > git-source: git-source-reefer-ml Params > context: simulator Service Account: reefer-simulator A new PipelineRun will be created. Click on the name of the running PipelineRun. From here, you can monitor the live running logs of the pipeline, as well as see what is running from the command-line with Kubectl ( kubectl get pods ). Note that the pipeline is actually running in pods deployed to the target namespace!","title":"Continuous Deployment"},{"location":"devops/cd/#continuous-deployment","text":"Our Continuous Deployment (CD) approach focuses on a GitOps-based deployment model, using Git as a single source of truth for the deployment, management, and operations of our running application components. In this model, we have the flexibility to use multiple open-source to apply the single source of truth from a given Git repository onto a desired cluster environment. More detail around the background of GitOps and how it differs from traditional deployment models can be found in this blog post from WeaveWorks. One of the main tools that we use in this space is a GitOps-focused continuous deployment project named ArgoCD . As documented by the IBM Garage for Cloud team, ArgoCD can monitor GitHub-based projects and apply changes stored in that repository's YAML files to a running Kubernetes-based cluster. We have documented our general ArgoCD Continuous Delivery workflow in the ibm-cloud-architecture/refarch-kc-gitops repository. The details of our ArgoCD-based GitOps deployments are covered in the ArgoCD deployments section below. Another DevOps tool which provides the opportunity to deployment applications via the GitOps methodology is Tekton . The Tekton Pipelines project provides a declarative language for defining and executing both CI and CD-style pipelines, all defined with common Kubernetes-like nomenclature. It even has the capability to kick off pipeline runs based off of GitHub webhooks. The details of our Tekton & Appsody deployments are covered in the Tekton & Appsody deployments section below.","title":"Continuous Deployment"},{"location":"devops/cd/#argocd-deployments","text":"Our main continuous deployment pattern operates on the same principle of \"zero-infrastructure overhead\" as our continuous integration implementations. This allows us to be agile, adaptable, and efficient in what we deploy where. ArgoCD is a perfect companion to this principle, as we do not need additional long-running CD infrastructure to monitor either a source environment or a target deployment environment. Our CI process sits with our code (on the same hosted infrastructure), while our CD process sits with the target deployment environment (on the same Kubernetes-based cluster). To utilize ArgoCD in this manner, we define a set of Kubernetes YAMLs generated from helm template commands, with environment, namespace, and cluster-specific parameters provided as needed. The details of generating those helm template YAMLs can be found in our main Application Components documentation. The templated YAMLs are generated with the names of the required ConfigMaps and Secrets specific to a namespace on the eventual target deployment cluster. This allows us to create a deployment artifact programmatically without exposing confidential and secret credentials via source code. Once these YAMLs are generated, they are checked in to the main GitOps repository for the project, under a new branch with a branch name in the format of <namespace>/<cluster> for ease of identification. These will then have a folder structure of /<component-name>/templates/<artifact-type>.yaml , with most components providing Deployment, Service, and Route artifact YAMLs. An ArgoCD application is then created on the ArgoCD deployment inside the target environment that can read from the GitOps repository. ArgoCD can also deploy between clusters, which does come in handy in certain use cases, but remember our squads main goal of \"zero-infrastructure overhead\" , so we deploy from ArgoCD into the same cluster it is deployed on the majority of the time. The ArgoCD application is a Custom-Resource Definition, comprising of the details necessary to determine the remote code repository URL, the branch of the code to use, the target namespace, and any formatting capabilities that are necessary. ArgoCD then handles automatically (or manually) syncing the deployments in the target namespace with the state that is described in the YAMLs on the specific branch in the GitOps repository. To keep in sync with the continuous integration implementation we have defined in Continuous integration , we have an additional GitHub Actions workflow defined in this repository that will update the YAML files contained in the repository with the latest microservice container images as they are modified, thus enabling a completely automated build-to-deployment lifecycle. Defined in the .github/workflows/update-gitops-deployments-(eda-integration).yaml workflow file, the workflow will scan the repository for all templated use of container images in Kubernetes Deployment files (and recently updated to be extensible to any YAML-based file!), search Docker Hub for the latest version of that container image, update the YAML file in-place, and check in the YAML updates back to the same repository and branch. This process is kicked off by the webhook Jobs mentioned in our CI process, as well as on a regularly-scheduled cron-like timer.","title":"ArgoCD deployments"},{"location":"devops/cd/#deploying-a-microservice-with-argocd","text":"You can deploy both the simulator and the scoring-mp microservices using ArgoCD in this reference implementation. The process is the same for either microservice, with minor parameter configuration being the only difference between the two.","title":"Deploying a microservice with ArgoCD"},{"location":"devops/cd/#prerequisites","text":"Ensure all necessary Kubernetes ConfigMaps and Secrets have been created in the namespace in which the application will be running. Ensure ArgoCD has been deployed to the local cluster with connectivity to the internet.","title":"Prerequisites"},{"location":"devops/cd/#using-the-argocd-dashboard","text":"Access the ArgoCD Dashboard via it's exposed Route in the argocd namespace. This should be in the form of https://argocd-server-argocd.apps.green.ocp.csplab.local . Depending upon your cluster and ArgoCD, you will have specific login credentials. Login as directed and click NEW APPLICATION . Enter the following parameters for either the SpringContainerMS or scoring-mp microservice and click CREATE . SpringContainerMS Application Name: springcontainerms Project: Select default from the drop-down Sync Policy: Select Automatic from the drop-down Repository URL: https://github.com/ibm-cloud-architecture/refarch-kc-gitops.git Revision: eda-sandbox/api-green-ocp-csplab-local (this is the branch in the GitOps repository for our specific project on our specific cluster) Path: springcontainerms Cluster: https://kubernetes.default.svc (this is our local cluster) Namespace: Your desired target namespace Check the Directory: Include subdirectories checkbox scoring-mp Application Name: scoring-mp Project: Select default from the drop-down Sync Policy: Select Automatic from the drop-down Repository URL: https://github.com/ibm-cloud-architecture/refarch-kc-gitops.git Revision: eda-sandbox/api-green-ocp-csplab-local (this is the branch in the GitOps repository for our specific project on our specific cluster) Path: scoring-mp Cluster: https://kubernetes.default.svc (this is our local cluster) Namespace: Your desired target namespace Check the Directory: Include subdirectories checkbox Once the application is successfully created inside ArgoCD, you can click the application tile to see the latest status of the ArgoCD-managed, GitOps-deployed microservice instance. It should begin synchronizing immediately upon creation. As ArgoCD applies the desired configuration to the cluster, you should see the pods of the microservice being created: kubectl get Pods","title":"Using the ArgoCD dashboard"},{"location":"devops/cd/#using-the-command-line-interface","text":"ArgoCD provides a command-line interface as well, however we are not covering that in this reference implementation. Once you are satisfied with the ArgoCD dashboard-based deployment pattern, you can reference the ArgoCD Getting Started docs for further details on using the ArgoCD CLI and CRD YAMLs for programmatic interaction.","title":"Using the command-line interface"},{"location":"devops/cd/#tekton-appsody-deployments","text":"We have also implemented some facets of the project deployment workflows using the Tekton Pipelines project and its inherent ease of support of the Appsody open-source developer experience project through the standard integration between the two built into the Kabanero open-source project, or more formally, the IBM Cloud Pak for Applications . Defined in the /scripts/tekton directory, we have a simple pipeline that will utilize the appsody deploy command to deploy the generated AppsodyApplication CRD YAML to the target environment. Similar to our ArgoCD-based deployments of Helm-generated, standard Kubernetes YAMLs, AppsodyApplication YAMLs can also be deployed through ArgoCD in a GitOps manner. However, for demonstration inside this project, additional capabilities are provided to showcase how we can utilize different pieces of the platform to deploy similar applications when different requirements are presented. Similar to ArgoCD, Tekton Pipelines run on the same cluster (and often in the same namespace!) as your running application code, thus allowing for more programmatic control over the deployment, management, operations, and existence of your application components. The key artifact that enables Tekton to deploy our Appsody-based refarch-reefer-ml/simulator microservice is the generated app-deploy.yaml file. The refarch-reefer-ml/simulator/app-deploy.yaml file was generated according to the appsody build command and then annotated with the required environment variables and metadata for successful operation in a given namespace, very similar to the pattern required for generating our Helm-templated YAMLs in the ArgoCD deployments section above. We then make use of the Appsody Operator to apply the AppsodyApplication to the target environment through the appsody deploy --no-build command. As documented in the Appsody Docs , we are able to take advantage of the pre-built container images available on Docker Hub and the annotated app-deploy.yaml file that is now a synonymous GitOps-like deployment artifact to quickly apply the change to the target namespace in the same cluster. Once the appsody deploy command is succesful, the Appsody Operator and Kubernetes takes care of the rest and reconciles the necessary underlying Kubernetes artifacts that are required to fulfill the requirements of serving up the application code in real-time!","title":"Tekton &amp; Appsody deployments"},{"location":"devops/cd/#deploying-the-simulator-microservice-with-tekton-appsody","text":"","title":"Deploying the simulator microservice with Tekton &amp; Appsody"},{"location":"devops/cd/#prerequisites_1","text":"Ensure all necessary Kubernetes ConfigMaps and Secrets have been created in the namespace in which the application will be running. Ensure an Appsody Operator has been configured to watch the namespace in which the application will be running. Create a new or modify an existing Service Account in the target namespace and bind the required API RBAC requirements for the Appsody Operator. Further details available in the Appsody Docs A sample YAML document has been provided via rbac-sa.yaml , with environment-specific updates to the Namespace and Service Account name fields being required. From the refarch-reefer-ml directory, import the Tekton pipeline artifacts: kubectl apply -f scripts/tekton/ Validate these items have been imported successfully by querying the cluster: kubectl get Pipeline,Task,PipelineResource","title":"Prerequisites"},{"location":"devops/cd/#using-the-command-line-interface_1","text":"Open the /scripts/tekton/manual/simulator-pipeline-run.yaml file in a text editor and ensure everything makes sense. Kick off a new pipeline run with the Kubernetes CLI: kubectl create -f scripts/tekton/manual/simulator-pipeline-run.yaml You can monitor the pipeline by common kubectl get and kubectl describe commands: kubectl get PipelineRun For further details on how to access the logs of a PipelineRun, reference the Tekton Pipelines documentation . Once the pipeline completes, you should see a deployed instance of the Simulator Appsody application: kubectl get Pods","title":"Using the command-line interface"},{"location":"devops/cd/#using-the-tekton-dashboard","text":"Access the Tekton Pipelines dashboard from your Kabanero installation or directly via the defined Route. This will be something similar to https://tekton-dashboard-tekton-pipelines.apps.green.ocp.csplab.local . Click PipelineRuns in the left-nav menu and click Create PipelineRun from the upper-right of the page. Enter the following parameters and click Create : Namespace: Target namespace for application deployment target Pipeline: appsody-deploy-pipeline PipelineResources > git-source: git-source-reefer-ml Params > context: simulator Service Account: reefer-simulator A new PipelineRun will be created. Click on the name of the running PipelineRun. From here, you can monitor the live running logs of the pipeline, as well as see what is running from the command-line with Kubectl ( kubectl get pods ). Note that the pipeline is actually running in pods deployed to the target namespace!","title":"Using the Tekton dashboard"},{"location":"devops/ci/","text":"Continuous Integration Our Continuous Integration (CI) approach is one of \"zero-infrastructure overhead\" . As such, we utilize GitHub Actions to build and push the microservice's associated container image to Docker Hub for public consumption. The GitHub Actions workflows are defined in the owning repository's .github/workflows/dockerbuild.yaml file. The Reefer Simulator microservice's CI implementation can be found via /.github/workflows/dockerbuild.yaml in the refarch-reefer-ml repository, while the SpringContainerMS microservice's CI implementation can be found via .github/workflows/dockerbuild.yaml in the refarch-kc-container-ms repository. For results of individual completed CI workflow actions, you can view the results via the Actions tab of a given repository. Overview of Continuous Integration workflows for this project The continuous integration workflow for our project looks in the figure below, which are detailing in next sections: On the left side, the developer uses his environment with Appsody CLI to build, and test his code, once tests run successfully, he commits and pushes the code to the master branch, then the github workflow triggers... 1 - Validate Docker Secrets The first job in each GitHub Actions workflow, validate-docker-secrets , ensures that all the necessary Secrets are defined on the repository under which the build action is running. Similar to Kubernetes Secrets, GitHub Repository Secrets allow you to store encrypted, sensitive information in a programmatically accessible way. Here is an example of such secret definitions: 2 - Build Component Images Appsody build for the Simulator microservice The simulator microservice is built using the Appsody open-source project, while leveraging the Python Flask Appsody Stack for its underlying framework. The project can be easily built in a local environment by issuing the appsody build commands, further documented on the Appsody site under Building and deploying . The second job in the Simulator microservice workflow, build-simulator-image , runs on a base Ubuntu container image, creates a new semantically-versioned tag (in the form of 0.1.2 ) for the repository, installs the latest Appsody CLI tools, performs the appsody build command with the appropriate parameters , tags the newly created version-specific image with latest as well, and pushes the image with both tags to the public Docker Hub repository defined by the aforementioned repository secrets. Container build for the Spring Container microservice The SpringContainerMS microservice is developed using the Spring Boot framework, compiled via maven and an associated pom.xml file, and packaged via a traditional Dockerfile. This project can be built locally via Maven and/or Docker, however it is recommended to consume the published container images that are a result of this continuous integration. The second job in the SpringContainerMS microservice github action workflow , build-springcontainer-image , runs on a base Ubuntu container image, creates a new semantically-versioned tag (in the form of 0.1.2 ) for the repository, performs a traditional docker build using the SpringContainerMS/Dockerfile , tags the newly created version-specific image with latest as well, and pushes the image with both tags to the public Docker Hub repository defined by the aforementioned repository secrets. 3 - GitOps Updates The final job, gitops-repo-webhook , is a linkage to our general continuous deployment process, which is GitOps-based and available via ibm-cloud-architecture/refarch-kc-gitops . This step performs a webhook call to our GitOps repository and notifies that repository's GitHub Actions that an update to one of its component's container images has been made and it should scan for the latest version of all the known container images and update the associated YAML files for environment updates. The repository action dispatcher triggers the git action workflow as defined here so the corresponding yaml files (appsody and helm configurations) can be updated (See this repository to understand the action update gitops). Here is an example of appsody.yaml automatically modified in the github repository: ... labels : image.opencontainers.org/title : reefer-simulator stack.appsody.dev/version : 0.1.6 name : reefer-simulator spec : applicationImage : ibmcase/kcontainer-reefer-simulator-appsody:0.1.26 Further description of this continuous deployment process is covered in Continuous Deployment note .","title":"Continuous Integration"},{"location":"devops/ci/#continuous-integration","text":"Our Continuous Integration (CI) approach is one of \"zero-infrastructure overhead\" . As such, we utilize GitHub Actions to build and push the microservice's associated container image to Docker Hub for public consumption. The GitHub Actions workflows are defined in the owning repository's .github/workflows/dockerbuild.yaml file. The Reefer Simulator microservice's CI implementation can be found via /.github/workflows/dockerbuild.yaml in the refarch-reefer-ml repository, while the SpringContainerMS microservice's CI implementation can be found via .github/workflows/dockerbuild.yaml in the refarch-kc-container-ms repository. For results of individual completed CI workflow actions, you can view the results via the Actions tab of a given repository.","title":"Continuous Integration"},{"location":"devops/ci/#overview-of-continuous-integration-workflows-for-this-project","text":"The continuous integration workflow for our project looks in the figure below, which are detailing in next sections: On the left side, the developer uses his environment with Appsody CLI to build, and test his code, once tests run successfully, he commits and pushes the code to the master branch, then the github workflow triggers...","title":"Overview of Continuous Integration workflows for this project"},{"location":"devops/ci/#1-validate-docker-secrets","text":"The first job in each GitHub Actions workflow, validate-docker-secrets , ensures that all the necessary Secrets are defined on the repository under which the build action is running. Similar to Kubernetes Secrets, GitHub Repository Secrets allow you to store encrypted, sensitive information in a programmatically accessible way. Here is an example of such secret definitions:","title":"1 - Validate Docker Secrets"},{"location":"devops/ci/#2-build-component-images","text":"","title":"2 - Build Component Images"},{"location":"devops/ci/#appsody-build-for-the-simulator-microservice","text":"The simulator microservice is built using the Appsody open-source project, while leveraging the Python Flask Appsody Stack for its underlying framework. The project can be easily built in a local environment by issuing the appsody build commands, further documented on the Appsody site under Building and deploying . The second job in the Simulator microservice workflow, build-simulator-image , runs on a base Ubuntu container image, creates a new semantically-versioned tag (in the form of 0.1.2 ) for the repository, installs the latest Appsody CLI tools, performs the appsody build command with the appropriate parameters , tags the newly created version-specific image with latest as well, and pushes the image with both tags to the public Docker Hub repository defined by the aforementioned repository secrets.","title":"Appsody build for the Simulator microservice"},{"location":"devops/ci/#container-build-for-the-spring-container-microservice","text":"The SpringContainerMS microservice is developed using the Spring Boot framework, compiled via maven and an associated pom.xml file, and packaged via a traditional Dockerfile. This project can be built locally via Maven and/or Docker, however it is recommended to consume the published container images that are a result of this continuous integration. The second job in the SpringContainerMS microservice github action workflow , build-springcontainer-image , runs on a base Ubuntu container image, creates a new semantically-versioned tag (in the form of 0.1.2 ) for the repository, performs a traditional docker build using the SpringContainerMS/Dockerfile , tags the newly created version-specific image with latest as well, and pushes the image with both tags to the public Docker Hub repository defined by the aforementioned repository secrets.","title":"Container build for the Spring Container microservice"},{"location":"devops/ci/#3-gitops-updates","text":"The final job, gitops-repo-webhook , is a linkage to our general continuous deployment process, which is GitOps-based and available via ibm-cloud-architecture/refarch-kc-gitops . This step performs a webhook call to our GitOps repository and notifies that repository's GitHub Actions that an update to one of its component's container images has been made and it should scan for the latest version of all the known container images and update the associated YAML files for environment updates. The repository action dispatcher triggers the git action workflow as defined here so the corresponding yaml files (appsody and helm configurations) can be updated (See this repository to understand the action update gitops). Here is an example of appsody.yaml automatically modified in the github repository: ... labels : image.opencontainers.org/title : reefer-simulator stack.appsody.dev/version : 0.1.6 name : reefer-simulator spec : applicationImage : ibmcase/kcontainer-reefer-simulator-appsody:0.1.26 Further description of this continuous deployment process is covered in Continuous Deployment note .","title":"3 - GitOps Updates"},{"location":"environments/cp4d/","text":"Cloud Pak For Data This repository does not address how to install and configure Cloud Pak for Data. The following source of information can be used to get a cluster up and running: Installing Cloud Pak for Data on Red Hat OpenShift Cloud pak playbook","title":"Cloud Pak for Data"},{"location":"environments/cp4d/#cloud-pak-for-data","text":"This repository does not address how to install and configure Cloud Pak for Data. The following source of information can be used to get a cluster up and running: Installing Cloud Pak for Data on Red Hat OpenShift Cloud pak playbook","title":"Cloud Pak For Data"},{"location":"environments/event-streams/","text":"Event Streams provisioning and configuration We cover a lot about Kafka or IBM Event Streams installation and configuration in the EDA reference architecture repository . In this short note we just highlight the steps to be done for deploying Event Streams on premise using Cloud Pak for integration. Deploying Event Streams from Cloud Pak for Integration The cloud pack for integration includes IBM Event Streams, the Kafka solution for on premise deployment. Once you have an Openshift cluster, you can install cloud pak for integration as presented in this tutorial . Then you can deploy Event streams with the default configuration of three broker cluster from the CP4I home page: For your own deployment you can follow the steps described in this tutorial and the Event Streams product documentation . Once you have your instance up and running, you need to get the URL for the brokers, the API key to access topics and the TLS certificate. Define the API key: The copy the broker URL and api key in the scripts/setenv.sh file under the OCP choice: OCP) export KAFKA_BROKERS=eventstream140-ibm-es-proxy-route-bootstrap-eventstreams.apps.green.ocp.csplab.local:443 export KAFKA_APIKEY=\"zb5Rv-81m11A0_\" export KAFKA_CERT=\"/project/useapp/simulator/certs/ocp/es-cert.pem\" And then download the pem and java key. We keep those files in the certs/ocp folder. As an alternate you can use Event Streams on Public Cloud. Event Streams on IBM Cloud Public We recommend creating the Event Stream service using the IBM Cloud catalog , you can also read our quick article on how to deploy Event Streams. With IBM Cloud deployment use the service credentials to create new credentials to get the Kafka brokers list, the admin URL and the api key needed to authenticate the consumers and the producers. For Event Streams on Openshift deployment, click to the connect to the cluster button to get the broker URL and to generate the API key: select the option to generate the key for all topics. Defines topics Create Kafka topics through Kubernetes Job automation In an effort to keep development systems as clean as possible and speed up deployment of various scenarios, our deployment tasks have been encapsulated in Kubernetes Jobs . These are runnable on any Kubernetes platform, including OpenShift. Following the configuration prerequisistes defined in the Backing Services documentation for using Kafka via IBM Event Streams on IBM Cloud or IBM Event Streams on OpenShift , you should already have the following Kubernetes ConfigMap & Secrets defined in your target namespace with the information available from the Connect to this service tab on the respective Event Streams service console: ConfigMap: kafka-brokers (in a comma-separated list) kubectl create configmap kafka-brokers --from-literal = brokers = 'host1.appdomain.cloud.com,host2.appdomain.cloud.com,...' Secret: eventstreams-apikey kubectl create secret generic eventstreams-apikey --from-literal = binding = '1a2...9z0' Secret: eventstreams-truststore-jks (this is only required when connecting to IBM Event Streams on OpenShift) kubectl create secret generic eventstreams-truststore-jks --from-file = ~/Downloads/es-cert.jks Event Streams Truststore password - this value is not contained in a Kubernetes Secret, but if using non-default settings in the Event Streams deployment, it should be verified that the password for the generated truststore file is still the default value of password . Review the /scripts/createKafkaTopics.yaml and the fields contained in the env section for optional parameters that can be modified when running the Job for non-default tasks. Create the create-kafka-topics Job from the root of the refarch-reefer-ml repository: kubectl apply -f scripts/createKafkaTopics.yaml You can tail the created pod's output to see the progress of the Kafka topic creation: kubectl logs -f --selector = job-name = create-kafka-topics Create Kafka topics manually through offering UI The following diagram illustrates the needed Kafka topics configured in IBM Cloud Event Stream service: For the telemetries we are now using 3 replicas. This is an example of configuration for Event Streams on openshift on premise:","title":"Event Streams environment"},{"location":"environments/event-streams/#event-streams-provisioning-and-configuration","text":"We cover a lot about Kafka or IBM Event Streams installation and configuration in the EDA reference architecture repository . In this short note we just highlight the steps to be done for deploying Event Streams on premise using Cloud Pak for integration.","title":"Event Streams provisioning and configuration"},{"location":"environments/event-streams/#deploying-event-streams-from-cloud-pak-for-integration","text":"The cloud pack for integration includes IBM Event Streams, the Kafka solution for on premise deployment. Once you have an Openshift cluster, you can install cloud pak for integration as presented in this tutorial . Then you can deploy Event streams with the default configuration of three broker cluster from the CP4I home page: For your own deployment you can follow the steps described in this tutorial and the Event Streams product documentation . Once you have your instance up and running, you need to get the URL for the brokers, the API key to access topics and the TLS certificate. Define the API key: The copy the broker URL and api key in the scripts/setenv.sh file under the OCP choice: OCP) export KAFKA_BROKERS=eventstream140-ibm-es-proxy-route-bootstrap-eventstreams.apps.green.ocp.csplab.local:443 export KAFKA_APIKEY=\"zb5Rv-81m11A0_\" export KAFKA_CERT=\"/project/useapp/simulator/certs/ocp/es-cert.pem\" And then download the pem and java key. We keep those files in the certs/ocp folder. As an alternate you can use Event Streams on Public Cloud.","title":"Deploying Event Streams from Cloud Pak for Integration"},{"location":"environments/event-streams/#event-streams-on-ibm-cloud-public","text":"We recommend creating the Event Stream service using the IBM Cloud catalog , you can also read our quick article on how to deploy Event Streams. With IBM Cloud deployment use the service credentials to create new credentials to get the Kafka brokers list, the admin URL and the api key needed to authenticate the consumers and the producers. For Event Streams on Openshift deployment, click to the connect to the cluster button to get the broker URL and to generate the API key: select the option to generate the key for all topics.","title":"Event Streams on IBM Cloud Public"},{"location":"environments/event-streams/#defines-topics","text":"","title":"Defines topics"},{"location":"environments/event-streams/#create-kafka-topics-through-kubernetes-job-automation","text":"In an effort to keep development systems as clean as possible and speed up deployment of various scenarios, our deployment tasks have been encapsulated in Kubernetes Jobs . These are runnable on any Kubernetes platform, including OpenShift. Following the configuration prerequisistes defined in the Backing Services documentation for using Kafka via IBM Event Streams on IBM Cloud or IBM Event Streams on OpenShift , you should already have the following Kubernetes ConfigMap & Secrets defined in your target namespace with the information available from the Connect to this service tab on the respective Event Streams service console: ConfigMap: kafka-brokers (in a comma-separated list) kubectl create configmap kafka-brokers --from-literal = brokers = 'host1.appdomain.cloud.com,host2.appdomain.cloud.com,...' Secret: eventstreams-apikey kubectl create secret generic eventstreams-apikey --from-literal = binding = '1a2...9z0' Secret: eventstreams-truststore-jks (this is only required when connecting to IBM Event Streams on OpenShift) kubectl create secret generic eventstreams-truststore-jks --from-file = ~/Downloads/es-cert.jks Event Streams Truststore password - this value is not contained in a Kubernetes Secret, but if using non-default settings in the Event Streams deployment, it should be verified that the password for the generated truststore file is still the default value of password . Review the /scripts/createKafkaTopics.yaml and the fields contained in the env section for optional parameters that can be modified when running the Job for non-default tasks. Create the create-kafka-topics Job from the root of the refarch-reefer-ml repository: kubectl apply -f scripts/createKafkaTopics.yaml You can tail the created pod's output to see the progress of the Kafka topic creation: kubectl logs -f --selector = job-name = create-kafka-topics","title":"Create Kafka topics through Kubernetes Job automation"},{"location":"environments/event-streams/#create-kafka-topics-manually-through-offering-ui","text":"The following diagram illustrates the needed Kafka topics configured in IBM Cloud Event Stream service: For the telemetries we are now using 3 replicas. This is an example of configuration for Event Streams on openshift on premise:","title":"Create Kafka topics manually through offering UI"},{"location":"environments/mongo/","text":"Preparing MongoDB on IBM Cloud Create the MongoDB service on IBM cloud using default configuration Figure 1: IBM Cloud Database Figure 2: Mongo DB default configuration Once created the Mongo instance can be found under the services in your resource list: Figure 3: Mongo DB service in resource list Add a Service credentials to get the mongodb.composed url: (something starting as mongodb://ibm_cloud_e154ff52_ed) the username and password. Figure 4: Mongo DB credentials Get the TLS certificate as pem file: ibmcloud cdb deployment-cacert gse-eda-mongodb > certs/mongodbca.pem If you never used Mongo Compass tool to access a Mongo DB, you can get started with this note.","title":"MongoDB on IBM Cloud"},{"location":"environments/mongo/#preparing-mongodb-on-ibm-cloud","text":"Create the MongoDB service on IBM cloud using default configuration Figure 1: IBM Cloud Database Figure 2: Mongo DB default configuration Once created the Mongo instance can be found under the services in your resource list: Figure 3: Mongo DB service in resource list Add a Service credentials to get the mongodb.composed url: (something starting as mongodb://ibm_cloud_e154ff52_ed) the username and password. Figure 4: Mongo DB credentials Get the TLS certificate as pem file: ibmcloud cdb deployment-cacert gse-eda-mongodb > certs/mongodbca.pem If you never used Mongo Compass tool to access a Mongo DB, you can get started with this note.","title":"Preparing MongoDB on IBM Cloud"},{"location":"environments/mongodb-compass/","text":"Use MongoDB Compass user interface First download from MongoDB download center Once started define a new connection to the IBM Cloud mongodb instance, by using the detail view so we can access the TLS settings. Once the connection is valid you can access the collection and browse or edit the data.","title":"Use MongoDB Compass user interface"},{"location":"environments/mongodb-compass/#use-mongodb-compass-user-interface","text":"First download from MongoDB download center Once started define a new connection to the IBM Cloud mongodb instance, by using the detail view so we can access the TLS settings. Once the connection is valid you can access the collection and browse or edit the data.","title":"Use MongoDB Compass user interface"},{"location":"environments/postgresql/","text":"PostgreSQL on IBM Cloud We are using Postgresql as a data source to persist Product and Container information. The Produts table is used in the predictive model construction. So we need to provision a Postgresql service in IBM Cloud. Use the product documentation to provision your own service. Here is a figure of the databases services in IBM Cloud as of December 2019. Using the default configuration the service is created in a minutes. Once done, we need to define the service credentials for the host, user, password and the certificate to be used by client appliations. Something like that: \"host=bd2d0216-0b7d-4575-8c0b-d2e934843e41.6131b73286f34215871dfad7254b4f7d.databases.appdomain.cloud port=31384 dbname=ibmclouddb user=ibm_cloud_c958... \"PGPASSWORD\": \"2d1c526.....3\" From these informations we need to define the POSTGRES environment variables in the scripts/setenv.sh file. (if not done before rename the scripts/setenv-tmpl.sh to scripts/setenv.sh ) POSTGRES_URL, POSTGRES_DBNAME, You also need to get the SSL certificate as a postgres.pem file, using the following ibmcloud CLI commands: ibmcloud login ibmcloud cdb deployment-cacert <database deployment name> Then in setenv.sh set POSTGRES_SSL_PEM variable to the path where to find this file ( export POSTGRES_SSL_PEM=\"./certs/postgres.pem\" ). Connect with psql To validate the access, we use a public postgres docker image to run psql using the defined environment variables. We have a script to start it: $ cd scripts $ ./startPsql.sh IBMCLOUD root@452074de376a:/# PGPASSWORD = $POSTGRES_PWD psql --host = $HOST --port = $PORT --username = $POSTGRES_USER --dbname = $POSTGRES_DB ibmclouddb = > List relations... ibmclouddb => \\d Next... You can use our tool to add some product definitions, for that see this note...","title":"Postgresql on IBM Cloud"},{"location":"environments/postgresql/#postgresql-on-ibm-cloud","text":"We are using Postgresql as a data source to persist Product and Container information. The Produts table is used in the predictive model construction. So we need to provision a Postgresql service in IBM Cloud. Use the product documentation to provision your own service. Here is a figure of the databases services in IBM Cloud as of December 2019. Using the default configuration the service is created in a minutes. Once done, we need to define the service credentials for the host, user, password and the certificate to be used by client appliations. Something like that: \"host=bd2d0216-0b7d-4575-8c0b-d2e934843e41.6131b73286f34215871dfad7254b4f7d.databases.appdomain.cloud port=31384 dbname=ibmclouddb user=ibm_cloud_c958... \"PGPASSWORD\": \"2d1c526.....3\" From these informations we need to define the POSTGRES environment variables in the scripts/setenv.sh file. (if not done before rename the scripts/setenv-tmpl.sh to scripts/setenv.sh ) POSTGRES_URL, POSTGRES_DBNAME, You also need to get the SSL certificate as a postgres.pem file, using the following ibmcloud CLI commands: ibmcloud login ibmcloud cdb deployment-cacert <database deployment name> Then in setenv.sh set POSTGRES_SSL_PEM variable to the path where to find this file ( export POSTGRES_SSL_PEM=\"./certs/postgres.pem\" ).","title":"PostgreSQL on IBM Cloud"},{"location":"environments/postgresql/#connect-with-psql","text":"To validate the access, we use a public postgres docker image to run psql using the defined environment variables. We have a script to start it: $ cd scripts $ ./startPsql.sh IBMCLOUD root@452074de376a:/# PGPASSWORD = $POSTGRES_PWD psql --host = $HOST --port = $PORT --username = $POSTGRES_USER --dbname = $POSTGRES_DB ibmclouddb = >","title":"Connect with psql"},{"location":"environments/postgresql/#list-relations","text":"ibmclouddb => \\d","title":"List relations..."},{"location":"environments/postgresql/#next","text":"You can use our tool to add some product definitions, for that see this note...","title":"Next..."},{"location":"infuse/build-run/","text":"An alternate approach is to setup a CI/CD pipeline We have adopted the Git Action to manage the continuous integration , and ArgoCD for the continuous deployment. The build process will build the following images: [https://hub.docker.com/repository/docker/ibmcase/kcontainer-reefer-simulator] Helm charts are added for the simulator and the scoring agent, using helm create command, and then the values.yaml and deployment.yaml files were updated to set environment variables and other parameters. Test sending a simulation control to the POST api The script sendSimulControl.sh is used for that. The usage looks like: sendSimulControl.sh hostname simultype (co2sensor | o2sensor | poweroff) containerID nb_of_records pwd refarch-reefer-ml ./scripts/sendSimulControl.sh reefersimulatorroute-reefershipmentsolution.apps.green-with-envy.ocp.csplab.local co2sensor C01 2000 If you use no argument for this script, it will send co2sensor control to the service running on our openshift cluster on IBM Cloud. Looking at the logs from the pod using oc logs reefersimulator-3-jdh2v you can see something like: \"POST /order HTTP/1.1\" 404 232 \"-\" \"curl/7.54.0\" {'containerID': 'c100', 'simulation': 'co2sensor', 'nb_of_records': 10, 'good_temperature': 4.4} Generating 10 Co2 metrics We will see how those events are processed in the next section. The predictive scoring agent Applying the same pattern as the simulation webapp, we implement a kafka consumer and producer in python that calls the serialized analytical model. The code in the scoring\\eventConsumer folder. Applying a TDD approach we start by a TestScoring.py class. import unittest from domain.predictservice import PredictService class TestScoreMetric ( unittest . TestCase ): def testCreation ( self ): serv = PredictService if __name__ == '__main__' : unittest . main () Use the same python environment with docker: . / startPythonEnv root @1 de81b16f940 : / # export PYTHONPATH =/ home / scoring / eventConsumer root @1 de81b16f940 : / # cd / home / scoring / eventConsumer root @1 de81b16f940 : / home / scoring / eventConsumer # python tests / TestScoring . py Test fails, so let add the scoring service with a constructor, and load the serialized pickle model (which was copied from the ml folder). import pickle class PredictService : def __init__ ( self , filename = \"domain/model_logistic_regression.pkl\" ): self . model = pickle . load ( open ( filename , \"rb\" ), encoding = 'latin1' ) def predict ( self , metricEvent ): TESTDATA = StringIO ( metricEvent ) data = pd . read_csv ( TESTDATA , sep = \",\" ) data . columns = data . columns . to_series () . apply ( lambda x : x . strip ()) X = data [ X = data [ FEATURES_NAMES ]] return self . model . predict ( X ) Next we need to test a predict on an event formated as a csv string. The test looks like: serv = PredictService() header=\"\"\"Timestamp, ID, Temperature(celsius), Target_Temperature(celsius), Power, PowerConsumption, ContentType, O2, CO2, Time_Door_Open, Maintenance_Required, Defrost_Cycle\"\"\" event=\"2019-04-01 T16:29 Z,1813, 101, 4.291843460900875,4.4,0,10.273342381017777,3,4334.920958996634,4.9631508046318755,1,0,6\"\"\" record=header+\"\\n\"+event print(serv.predict(record)) So the scoring works, now we need to code the scoring application that will be deployed to Openshift cluster, and which acts as a consumer of container metrics events and a producer container events. The Scoring Agent code of this app is ScoringAgent.py module. It starts a consumer to get messages from Kafka. And when a message is received, it needs to do some data extraction and transformation and then use the predictive service. During the tests we have issue in the data quality, so it is always a good practice to add a validation function to assess if all the records are good. For production, this code needs to be enhanced for better error handling an reporting. Run locally Under scoring\\eventConsumer folder, set the environment variables for KAFKA using the commands below: (It uses event streams on IBM Cloud) export KAFKA_BROKERS=broker-3.eventstreams.cloud.ibm.com:9093,broker-1.eventstreams.cloud.ibm.com:9093,broker-0.eventstreams.cloud.ibm.com:9093,broker-5.eventstreams.cloud.ibm.com:9093,broker-2.eventstreams.cloud.ibm.com:9093,broker-4.eventstreams.cloud.ibm.com:9093 export KAFKA_APIKEY=\"set-api-key-for-eventstreams-on-cloud\" docker run -e KAFKA_BROKERS=$KAFKA_BROKERS -e KAFKA_APIKEY=$KAFKA_APIKEY -v $(pwd)/..:/home -ti ibmcase/python bash -c \"cd /home/scoring && export PYTHONPATH=/home && python ScoringAgent.py\" Scoring: Build and run on Openshift The first time we need to add the application to the existing project, run the following command: oc new-app python:latest~https://github.com/ibm-cloud-architecture/refarch-reefer-ml.git --context-dir=scoring/eventConsumer --name reeferpredictivescoring This command will run a source to image, build all the needed yaml files for the kubernetes deployment and start the application in a pod. It use the --context flag to define what to build and run. With this capability we can use the same github repository for different sub component. As done for simulator, the scoring service needs environment variables. We can set them using the commands oc set env dc/reeferpredictivescoring KAFKA_BROKERS=$KAFKA_BROKERS oc set env dc/reeferpredictivescoring KAFKA_APIKEY=$KAFKA_APIKEY oc set env dc/reeferpredictivescoring KAFKA_CERT=/opt/app-root/src/es-cert.pem but we have added a script for you to do so. This script needs only to be run at the first deployment. It leverage the common setenv scripts: ../scripts/defEnvVarInOpenShift.sh The list of running pods should show the build pods for this application: oc get pods reeferpredictivescoring-1-build 1/1 Running 0 24s To run the build again after commit code to github: oc start-build reeferpredictivescoring # or from local file system oc start-build reeferpredictivescoring --from-file=. To see the log: oc logs reeferpredictivescoring-2-rxr6j To be able to run on Openshift, the APP_FILE environment variable has to be set to ScoringApp.py. This can be done in the environment file under the .s2i folder. The scoring service has no API exposed to the external world, so we do not need to create a Route or ingress. See the integration test section to see a demonstration of the solution end to end. Build docker images For the scoring agent: # scoring folder Run kafka on your laptop For development purpose, you can also run kafka, zookeeper and postgresql and the solution on your laptop. For that read this readme for details.","title":"Build and run the solution on Openshift"},{"location":"infuse/build-run/#an-alternate-approach-is-to-setup-a-cicd-pipeline","text":"We have adopted the Git Action to manage the continuous integration , and ArgoCD for the continuous deployment. The build process will build the following images: [https://hub.docker.com/repository/docker/ibmcase/kcontainer-reefer-simulator] Helm charts are added for the simulator and the scoring agent, using helm create command, and then the values.yaml and deployment.yaml files were updated to set environment variables and other parameters.","title":"An alternate approach is to setup a CI/CD pipeline"},{"location":"infuse/build-run/#test-sending-a-simulation-control-to-the-post-api","text":"The script sendSimulControl.sh is used for that. The usage looks like: sendSimulControl.sh hostname simultype (co2sensor | o2sensor | poweroff) containerID nb_of_records pwd refarch-reefer-ml ./scripts/sendSimulControl.sh reefersimulatorroute-reefershipmentsolution.apps.green-with-envy.ocp.csplab.local co2sensor C01 2000 If you use no argument for this script, it will send co2sensor control to the service running on our openshift cluster on IBM Cloud. Looking at the logs from the pod using oc logs reefersimulator-3-jdh2v you can see something like: \"POST /order HTTP/1.1\" 404 232 \"-\" \"curl/7.54.0\" {'containerID': 'c100', 'simulation': 'co2sensor', 'nb_of_records': 10, 'good_temperature': 4.4} Generating 10 Co2 metrics We will see how those events are processed in the next section.","title":"Test sending a simulation control to the POST api"},{"location":"infuse/build-run/#the-predictive-scoring-agent","text":"Applying the same pattern as the simulation webapp, we implement a kafka consumer and producer in python that calls the serialized analytical model. The code in the scoring\\eventConsumer folder. Applying a TDD approach we start by a TestScoring.py class. import unittest from domain.predictservice import PredictService class TestScoreMetric ( unittest . TestCase ): def testCreation ( self ): serv = PredictService if __name__ == '__main__' : unittest . main () Use the same python environment with docker: . / startPythonEnv root @1 de81b16f940 : / # export PYTHONPATH =/ home / scoring / eventConsumer root @1 de81b16f940 : / # cd / home / scoring / eventConsumer root @1 de81b16f940 : / home / scoring / eventConsumer # python tests / TestScoring . py Test fails, so let add the scoring service with a constructor, and load the serialized pickle model (which was copied from the ml folder). import pickle class PredictService : def __init__ ( self , filename = \"domain/model_logistic_regression.pkl\" ): self . model = pickle . load ( open ( filename , \"rb\" ), encoding = 'latin1' ) def predict ( self , metricEvent ): TESTDATA = StringIO ( metricEvent ) data = pd . read_csv ( TESTDATA , sep = \",\" ) data . columns = data . columns . to_series () . apply ( lambda x : x . strip ()) X = data [ X = data [ FEATURES_NAMES ]] return self . model . predict ( X ) Next we need to test a predict on an event formated as a csv string. The test looks like: serv = PredictService() header=\"\"\"Timestamp, ID, Temperature(celsius), Target_Temperature(celsius), Power, PowerConsumption, ContentType, O2, CO2, Time_Door_Open, Maintenance_Required, Defrost_Cycle\"\"\" event=\"2019-04-01 T16:29 Z,1813, 101, 4.291843460900875,4.4,0,10.273342381017777,3,4334.920958996634,4.9631508046318755,1,0,6\"\"\" record=header+\"\\n\"+event print(serv.predict(record)) So the scoring works, now we need to code the scoring application that will be deployed to Openshift cluster, and which acts as a consumer of container metrics events and a producer container events. The Scoring Agent code of this app is ScoringAgent.py module. It starts a consumer to get messages from Kafka. And when a message is received, it needs to do some data extraction and transformation and then use the predictive service. During the tests we have issue in the data quality, so it is always a good practice to add a validation function to assess if all the records are good. For production, this code needs to be enhanced for better error handling an reporting.","title":"The predictive scoring agent"},{"location":"infuse/build-run/#run-locally","text":"Under scoring\\eventConsumer folder, set the environment variables for KAFKA using the commands below: (It uses event streams on IBM Cloud) export KAFKA_BROKERS=broker-3.eventstreams.cloud.ibm.com:9093,broker-1.eventstreams.cloud.ibm.com:9093,broker-0.eventstreams.cloud.ibm.com:9093,broker-5.eventstreams.cloud.ibm.com:9093,broker-2.eventstreams.cloud.ibm.com:9093,broker-4.eventstreams.cloud.ibm.com:9093 export KAFKA_APIKEY=\"set-api-key-for-eventstreams-on-cloud\" docker run -e KAFKA_BROKERS=$KAFKA_BROKERS -e KAFKA_APIKEY=$KAFKA_APIKEY -v $(pwd)/..:/home -ti ibmcase/python bash -c \"cd /home/scoring && export PYTHONPATH=/home && python ScoringAgent.py\"","title":"Run locally"},{"location":"infuse/build-run/#scoring-build-and-run-on-openshift","text":"The first time we need to add the application to the existing project, run the following command: oc new-app python:latest~https://github.com/ibm-cloud-architecture/refarch-reefer-ml.git --context-dir=scoring/eventConsumer --name reeferpredictivescoring This command will run a source to image, build all the needed yaml files for the kubernetes deployment and start the application in a pod. It use the --context flag to define what to build and run. With this capability we can use the same github repository for different sub component. As done for simulator, the scoring service needs environment variables. We can set them using the commands oc set env dc/reeferpredictivescoring KAFKA_BROKERS=$KAFKA_BROKERS oc set env dc/reeferpredictivescoring KAFKA_APIKEY=$KAFKA_APIKEY oc set env dc/reeferpredictivescoring KAFKA_CERT=/opt/app-root/src/es-cert.pem but we have added a script for you to do so. This script needs only to be run at the first deployment. It leverage the common setenv scripts: ../scripts/defEnvVarInOpenShift.sh The list of running pods should show the build pods for this application: oc get pods reeferpredictivescoring-1-build 1/1 Running 0 24s To run the build again after commit code to github: oc start-build reeferpredictivescoring # or from local file system oc start-build reeferpredictivescoring --from-file=. To see the log: oc logs reeferpredictivescoring-2-rxr6j To be able to run on Openshift, the APP_FILE environment variable has to be set to ScoringApp.py. This can be done in the environment file under the .s2i folder. The scoring service has no API exposed to the external world, so we do not need to create a Route or ingress. See the integration test section to see a demonstration of the solution end to end.","title":"Scoring: Build and run on Openshift"},{"location":"infuse/build-run/#build-docker-images","text":"For the scoring agent: # scoring folder","title":"Build docker images"},{"location":"infuse/build-run/#run-kafka-on-your-laptop","text":"For development purpose, you can also run kafka, zookeeper and postgresql and the solution on your laptop. For that read this readme for details.","title":"Run kafka on your laptop"},{"location":"infuse/dev-scoring/","text":"Develop the scoring agent with Cloud Pak for Application We have two approaches to build the predictive scoring agent: One using a python Flask app listening to telemetry events coming from Kafka, run the predictive scoring within the same app. The model was developed with Jupiter notebook and serialized with pickle , so it responds in microseconds and generates anomaly to a second kafka topic. The code is under scoring folder, and uses essentially open source components. One using Java MicroProfile 3.0 with the Reactive Messaging annotations, to consume telemetry events and call a remote predictive scoring service, developed and deployed within Cloud Pak for Data. The code is under scoping-mp folder. The scoring service needs to use an analytics scoring model built using machine learning techniques, and serialized so that it can be loaded in memory. Java MicroProfile 3.0 with Cloud Pak for Application In this version we, we would like to introduce the new MicroProfile Reactive Messaging feature that allows you to build Reactive Systems. But, what is a Reactive System? Reactive Systems Reactive Systems provide an architecture style to deliver responsive systems. By infusing asynchronous messaging passing at the core of the system, applications enforcing the reactive system\u2019s characteristics are inherently resilient and become more elastic by scaling up and down the number of message consumers. Microservices as part of reactive systems interact using messages. The location and temporal decoupling, promoted by this interaction mechanism, enable numerous benefits such as: Better failure handling as the temporal decoupling enables message brokers to resend or reroute messages in the case of remote service failures. Improved elasticity as under fluctuating load the system can decide to scale up and down some of the microservices. The ability to introduce new features more easily as components are more loosely coupled by receiving and publishing messages. The MicroProfile Reactive Messaging specification aims to deliver applications embracing the characteristics of reactive systems. Use Cases MicroProfile Reactive Messaging aims to provide a way to connect event-driven microservices. The key characteristics of the specification make it versatile and suitable for building different types of architecture and applications. First, asynchronous interactions with different services and resources can be implemented using Reactive Messaging. Typically, asynchronous database drivers can be used in conjunction with Reactive Messaging to read and write into a data store in a non-blocking and asynchronous manner. When building microservices, the CQRS and event-sourcing patterns provide an answer to the data sharing between microservices. Reactive Messaging can also be used as the foundation to CQRS and Event-Sourcing mechanism, as these patterns embrace message-passing as core communication pattern. IOT applications, dealing with events from various devices, and data streaming applications can also be implemented using Reactive Messaging. The application receives events or messages, process them, transform them, and may forward them to another microservices. It allows for more fluid architecture for building data-centric applications. MicroProfile Reactive Messaging MicroProfile Reactive Messaging provides a very easy-to-use way to send, receive, and process messages and is well-suited to writing applications that process streams of events. With MicroProfile Reactive Messaging, you annotate application beans' methods and, under the covers, OpenLiberty (in our specific case) converts these to reactive streams -compatible publishers, subscribers and processors and connects them up to each other. Although sending messages within our application is nice, it\u2019s more useful to be able to send and receive messages from other systems. For this, MicroProfile Reactive Messaging also provides a Connector API to allow your methods to be connected to external messaging systems. Open Liberty includes the liberty-kafka connector for sending and receiving messages from an Apache Kafka broker. It is important to understand that MicroProfile Reactive Messaging does not contain an implementation itself but only provides the specified API, a TCK and documentation. It is down to application server or external libraries to provide such implementation. For instance, Open Liberty 19.0.0.9 provides a full implementation of MicroProfile Reactive Messaging 1.0. OpenLiberty's implementation of the MicroProfile Reactive Messaging capabilities is based on SmallRye Reactive Messaging . Implement Now that we have introduce what reactive systems are, what the use cases for these are and how the MicroProfile Reactive Messaging feature aims to help in constructing these, let's have a look at how we have implemented our scoring agent. First of all, we have based our implementation on MicroProfile as the standardization for building microservices based applications. The application server we have chosen, that provides full MicroProfile support, is OpenLiberty . As already introduced, the scoring agent will react to a stream of data, in the form of messaging being sent from an IOT device in the Reefer Containers to Kafka (through an MQTT connector), which is used as our event backbone. The scoring agent will react to that stream of data being sent into a kafka topic through the MicroProfile Reactive Messaging feature that OpenLiberty implements and supports. It will do some computation for each of the messages coming as a stream of data, which in this case is to call the container anomaly predictive system, and send out a container anomaly message a to specific kafka topic where the Reefer Container EDA reference application listens to and will act in consequence. If you have not yet realise, this scenario fits into the IOT applications use case described in the earlier section . The main chunk of code that implements the scoring agent use case just described looks like this: @Incoming ( \"reefer-telemetry\" ) @Outgoing ( \"containers\" ) @Acknowledgment ( Acknowledgment . Strategy . MANUAL ) public PublisherBuilder < Message < String >> processTelemetry ( Message < String > message ) { // Get the message as String String input = message . getPayload (); Gson g = new Gson (); // Parsing the message into a TelemetryEvent Java Object TelemetryEvent te = g . fromJson ( input , TelemetryEvent . class ); // Getting the Telemetry out of the event. Telemetry t = new Telemetry ( te . getPayload ()); // Calling the Anomaly Scoring service ScoringResult scoringResult = scoringClient . callAnomalyScoring ( t ); // Getting the Scoring Prediction Results ScoringPredictionValues result = scoringResult . getScoringPredictionValues (); Boolean anomaly = result . getPrediction (). equalsIgnoreCase ( \"Issue\" ); if (! anomaly ){ System . out . println ( \"No container anomaly\" ); message . ack (); // All processing of this message is done, ack it now return ReactiveStreams . empty (); } else { System . out . println ( \"A container anomaly has been predicted. Therefore, sending a ContainerAnomaly Event to the appropriate topic\" ); ContainerAnomalyEvent cae = new ContainerAnomalyEvent ( te . getContainerID (), te . getTimestamp (), t ); System . out . println ( \"ContainerAnomalyEvent object sent: \" + cae . toString ()); // This message will be sent on, create a new message which acknowledges the incoming message when it is acked return ReactiveStreams . of ( Message . of ( cae . toString (), () -> message . ack ())); } } where we are consuming/producing the data from/to Kafka by simply annotating our method with @Incoming and @Outgoing and leaving the rest of the magic required to create the reactive streams to the MicroProfile Reactive Messaging implementation of the OpenLiberty server does. Although there is certain magic in this process, there is still some configuration we need to provide OpenLiberty with in order for it to properly create the reactive streams. Such configuration is provided in the microprofile-configuration.properties file: # Config specific to reefer-telemetry kafka topic mp.messaging.incoming.reefer-telemetry.connector = liberty-kafka mp.messaging.incoming.reefer-telemetry.group.id = reefer-telemetry-reactive mp.messaging.incoming.reefer-telemetry.topic = reefer-telemetry mp.messaging.incoming.reefer-telemetry.key.deserializer = org.apache.kafka.common.serialization.StringDeserializer mp.messaging.incoming.reefer-telemetry.value.deserializer = org.apache.kafka.common.serialization.StringDeserializer # Config specific to containers kafka topic mp.messaging.outgoing.containers.connector = liberty-kafka mp.messaging.outgoing.containers.group.id = containers-reactive mp.messaging.outgoing.containers.topic = containers mp.messaging.outgoing.containers.key.serializer = org.apache.kafka.common.serialization.StringSerializer mp.messaging.outgoing.containers.value.serializer = org.apache.kafka.common.serialization.StringSerializer #### Config shared between all kafka connections # bootstrap server is the only config needed for plain insecure local kafka instance mp.messaging.connector.liberty-kafka.bootstrap.servers = where we can see the configuration for each of the incoming and outgoing reactive streams based on their names (reefer-telemetry and containers) that, in fact, match with the Kafka topics these reactive streams will be consuming/producing from/to. We also specify the serializer/deserializer we want MicroProfile Reactive Messaging to apply to our reactive streams for data processing but more importantly is the fact that we are telling OpenLiberty that the connector to be used is the liberty-kafka connector. Otherwise, OpenLiberty would not be able to interact with Kafka. Finally, we are providing to the liberty-kafka connector the appropriate configuration for OpenLiberty to successfully connect and consume/produce data to Kafka. In the example above we are providing the Kafka bootstrap server but there are other properties that can be set here (such as security) that can be found in the MicroProfile Reactive Messaging Specification Build In order to be able to run our application, we first need to configure our OpenLiberty server to make use of the MicroProfile Reactive Messaging and MicroProfile Reactive Streams features it supports. Top do so, we specify such features in the server.xml file: <server description= \"Liberty server\" > <featureManager> <feature> microProfile-3.0 </feature> <!-- Reactive messaging and reactive streams --> <feature> mpReactiveMessaging-1.0 </feature> <feature> mpReactiveStreams-1.0 </feature> </featureManager> ... </server> Secondly, we need to configure our build process through the pom.xml file (we are using Maven to build our application) to compile the code and build the resulting application with: OpenLiberty as the target application server to run our application: <plugin> <groupId> net.wasdev.wlp.maven.plugins </groupId> <artifactId> liberty-maven-plugin </artifactId> <configuration> <assemblyArtifact> <groupId> ${liberty.groupId} </groupId> <artifactId> ${liberty.artifactId} </artifactId> <version> ${version.openliberty-runtime} </version> <type> zip </type> </assemblyArtifact> </configuration> </plugin> MicroProfile as the Java framework: <dependency> <groupId> org.eclipse.microprofile </groupId> <artifactId> microprofile </artifactId> <version> 3.0 </version> <type> pom </type> <scope> provided </scope> </dependency> MicroProfile Reactive Messaging and MicroProfile Reactive Streams capabilities: <dependency> <groupId> org.eclipse.microprofile.reactive.messaging </groupId> <artifactId> microprofile-reactive-messaging-api </artifactId> <version> 1.0 </version> <scope> provided </scope> </dependency> <dependency> <groupId> org.eclipse.microprofile.reactive-streams-operators </groupId> <artifactId> microprofile-reactive-streams-operators-api </artifactId> <version> 1.0.1 </version> <scope> provided </scope> </dependency> Apache Kafka Client: <dependency> <groupId> org.apache.kafka </groupId> <artifactId> kafka-clients </artifactId> <version> 2.3.0 </version> </dependency> Deploy Create a Kubernetes Secret for the required bootstrap.properties file which will pass the Kafka configuration details to the Reactive Messaging components. These values are the same as other existing ConfigMaps and Secrets, however they cannot be used in the same manner due to the order that they are required to be in the system when Liberty starts up. Download bootstrap.properties.tmpl Set mp.messaging.connector.liberty-kafka.bootstrap.servers to the comma-separated list of Kafka brokers. (This is the same value as the kafka-brokers ConfigMap.) Update mp.messaging.connector.liberty-kafka.sasl.jaas.config to replace the API-KEY placeholder value with the Event Streams API key. (This is the same value as the eventstreams-apikey Secret.) Create the Secret via the following command: kubectl create secret generic openliberty-bootstrap --from-file = bootstrap.properties = ./bootstrap.properties.tmpl Create a Kubernetes ConfigMap and Secret for the Cloud Pak For Data remote endpoints. ConfigMap: kubectl create configmap predictive-model-configmap --from-literal = baseURL = 'https://zen-cpd-zen.demo.ibmcloudpack.com/' --from-literal = predictionURL = '/v4/deployments/fb03738c-1234-abcd-wxyz-7e66106ee51c/predictions' Secret: kubectl create secret generic predictive-model-secret --from-literal = user = 'replace-with-your-username' --from-literal = password = 'replace-with-your-password' Create the necessary YAML files from the scoring-mp Helm Chart. A sample command is provided below: helm template --output-dir . --set serviceAccountName = reefer-simulator --set route.enabled = true chart/scoringmp kubectl apply -f ./scoringmp/template Run Once we have our application running, we can confirm the following by looking at the application logs: It is reacting to a data stream from the reefer-telemetry Kafka topic. It is calling the container anomaly predictive system. It is producing container anomaly events, when needed, into the containers Kafka topic. [AUDIT ] CWWKF0011I: The defaultServer server is ready to run a smarter planet. The defaultServer server started in 97.081 seconds. Received message: {\"containerID\": \"1111\", \"payload\": \"('1111', '2020-01-15 17:59:45', 'P05', 5.02702153, 5., 20.52035697, 2.62176459, 0, 1, 5, 21.56977522, 75.97754859, 39.85714797, 4.74727473, True, True, True, '37.8226902168957', '-122.324895', 0)\", \"timestamp\": \"2020-01-15 17:59:45\", \"type\": \"ReeferTelemetries\"} Getting Authentication token for the prediction service... IAM service returned: 200 Payload for the prediction service call: {\"input_data\":[{\"fields\":[\"temperature\",\"target_temperature\",\"ambiant_temperature\",\"kilowatts\",\"time_door_open\",\"defrost_cycle\",\"oxygen_level\",\"nitrogen_level\",\"humidity_level\",\"target_humidity_level\",\"carbon_dioxide_level\",\"fan_1\",\"fan_2\",\"fan_3\"],\"values\":[[5.02702153,5.0,20.52035697,2.62176459,0.0,5,21.56977522,75.97754859,39.85714797,39.85714797,4.74727473,true,true,true]]}]} This is the prediction: NoIssue This is the probability: [0.0010803937911987305,0.9989196062088013] No container anomaly Received message: {\"containerID\": \"1111\", \"payload\": \"('1111', '2020-01-15 17:59:45', 'P05', 5.02702153, 5., 20.52035697, 2.62176459, 0, 1, 5, 21.56977522, 75.97754859, 39.85714797, 4.74727473, True, True, True, '37.8226902168957', '-122.324895', 0)\", \"timestamp\": \"2020-01-15 17:59:45\", \"type\": \"ReeferTelemetries\"} Payload for the prediction service call: {\"input_data\":[{\"fields\":[\"temperature\",\"target_temperature\",\"ambiant_temperature\",\"kilowatts\",\"time_door_open\",\"defrost_cycle\",\"oxygen_level\",\"nitrogen_level\",\"humidity_level\",\"target_humidity_level\",\"carbon_dioxide_level\",\"fan_1\",\"fan_2\",\"fan_3\"],\"values\":[[5.02702153,5.0,20.52035697,2.62176459,0.0,5,21.56977522,75.97754859,39.85714797,39.85714797,4.74727473,true,true,true]]}]} This is the prediction: Issue This is the probability: [0.0010803937911987305,0.9989196062088013] A container anomaly has been predicted. Therefore, sending a ContainerAnomaly Event to the appropriate topic ContainerAnomalyEvent object sent: {containerID: 1111, timestamp: 1579111185000, type: ContainerAnomaly, payload: {temperature: 5.02702153, target_temperature: 5.0, ambiant_temperature: 20.52035697, kilowatts: 2.62176459, time_door_open: 0.0, content_type: 1, defrost_cycle: 5, oxygen_level: 21.56977522, nitrogen_level: 75.97754859, humidity_level: 39.85714797, carbon_dioxide_level: 4.74727473, fan_1: true, fan_2: true, fan_3: true, latitude: 37.8226902168957, longitude: -122.324895}} Deploying the model using Watson Machine Learning TODO Cloud Pak model deployment Further Readings Appsody for cloud native development Appsody microprofile stack Cloud Pak for Application demo video Use Codewind for VScode Reactive messaging between microservices with MicroProfile on Open Liberty 19.0.0.9 Sending and receiving messages between microservices with MicroProfile Reactive Messaging MicroProfile Reactive Messaging Specification Reactive Messaging for MicroProfile Reactive Streams","title":"Develop the scoring app with event messaging and microprofile"},{"location":"infuse/dev-scoring/#develop-the-scoring-agent-with-cloud-pak-for-application","text":"We have two approaches to build the predictive scoring agent: One using a python Flask app listening to telemetry events coming from Kafka, run the predictive scoring within the same app. The model was developed with Jupiter notebook and serialized with pickle , so it responds in microseconds and generates anomaly to a second kafka topic. The code is under scoring folder, and uses essentially open source components. One using Java MicroProfile 3.0 with the Reactive Messaging annotations, to consume telemetry events and call a remote predictive scoring service, developed and deployed within Cloud Pak for Data. The code is under scoping-mp folder. The scoring service needs to use an analytics scoring model built using machine learning techniques, and serialized so that it can be loaded in memory.","title":"Develop the scoring agent with Cloud Pak for Application"},{"location":"infuse/dev-scoring/#java-microprofile-30-with-cloud-pak-for-application","text":"In this version we, we would like to introduce the new MicroProfile Reactive Messaging feature that allows you to build Reactive Systems. But, what is a Reactive System?","title":"Java MicroProfile 3.0 with Cloud Pak for Application"},{"location":"infuse/dev-scoring/#reactive-systems","text":"Reactive Systems provide an architecture style to deliver responsive systems. By infusing asynchronous messaging passing at the core of the system, applications enforcing the reactive system\u2019s characteristics are inherently resilient and become more elastic by scaling up and down the number of message consumers. Microservices as part of reactive systems interact using messages. The location and temporal decoupling, promoted by this interaction mechanism, enable numerous benefits such as: Better failure handling as the temporal decoupling enables message brokers to resend or reroute messages in the case of remote service failures. Improved elasticity as under fluctuating load the system can decide to scale up and down some of the microservices. The ability to introduce new features more easily as components are more loosely coupled by receiving and publishing messages. The MicroProfile Reactive Messaging specification aims to deliver applications embracing the characteristics of reactive systems.","title":"Reactive Systems"},{"location":"infuse/dev-scoring/#use-cases","text":"MicroProfile Reactive Messaging aims to provide a way to connect event-driven microservices. The key characteristics of the specification make it versatile and suitable for building different types of architecture and applications. First, asynchronous interactions with different services and resources can be implemented using Reactive Messaging. Typically, asynchronous database drivers can be used in conjunction with Reactive Messaging to read and write into a data store in a non-blocking and asynchronous manner. When building microservices, the CQRS and event-sourcing patterns provide an answer to the data sharing between microservices. Reactive Messaging can also be used as the foundation to CQRS and Event-Sourcing mechanism, as these patterns embrace message-passing as core communication pattern. IOT applications, dealing with events from various devices, and data streaming applications can also be implemented using Reactive Messaging. The application receives events or messages, process them, transform them, and may forward them to another microservices. It allows for more fluid architecture for building data-centric applications.","title":"Use Cases"},{"location":"infuse/dev-scoring/#microprofile-reactive-messaging","text":"MicroProfile Reactive Messaging provides a very easy-to-use way to send, receive, and process messages and is well-suited to writing applications that process streams of events. With MicroProfile Reactive Messaging, you annotate application beans' methods and, under the covers, OpenLiberty (in our specific case) converts these to reactive streams -compatible publishers, subscribers and processors and connects them up to each other. Although sending messages within our application is nice, it\u2019s more useful to be able to send and receive messages from other systems. For this, MicroProfile Reactive Messaging also provides a Connector API to allow your methods to be connected to external messaging systems. Open Liberty includes the liberty-kafka connector for sending and receiving messages from an Apache Kafka broker. It is important to understand that MicroProfile Reactive Messaging does not contain an implementation itself but only provides the specified API, a TCK and documentation. It is down to application server or external libraries to provide such implementation. For instance, Open Liberty 19.0.0.9 provides a full implementation of MicroProfile Reactive Messaging 1.0. OpenLiberty's implementation of the MicroProfile Reactive Messaging capabilities is based on SmallRye Reactive Messaging .","title":"MicroProfile Reactive Messaging"},{"location":"infuse/dev-scoring/#implement","text":"Now that we have introduce what reactive systems are, what the use cases for these are and how the MicroProfile Reactive Messaging feature aims to help in constructing these, let's have a look at how we have implemented our scoring agent. First of all, we have based our implementation on MicroProfile as the standardization for building microservices based applications. The application server we have chosen, that provides full MicroProfile support, is OpenLiberty . As already introduced, the scoring agent will react to a stream of data, in the form of messaging being sent from an IOT device in the Reefer Containers to Kafka (through an MQTT connector), which is used as our event backbone. The scoring agent will react to that stream of data being sent into a kafka topic through the MicroProfile Reactive Messaging feature that OpenLiberty implements and supports. It will do some computation for each of the messages coming as a stream of data, which in this case is to call the container anomaly predictive system, and send out a container anomaly message a to specific kafka topic where the Reefer Container EDA reference application listens to and will act in consequence. If you have not yet realise, this scenario fits into the IOT applications use case described in the earlier section . The main chunk of code that implements the scoring agent use case just described looks like this: @Incoming ( \"reefer-telemetry\" ) @Outgoing ( \"containers\" ) @Acknowledgment ( Acknowledgment . Strategy . MANUAL ) public PublisherBuilder < Message < String >> processTelemetry ( Message < String > message ) { // Get the message as String String input = message . getPayload (); Gson g = new Gson (); // Parsing the message into a TelemetryEvent Java Object TelemetryEvent te = g . fromJson ( input , TelemetryEvent . class ); // Getting the Telemetry out of the event. Telemetry t = new Telemetry ( te . getPayload ()); // Calling the Anomaly Scoring service ScoringResult scoringResult = scoringClient . callAnomalyScoring ( t ); // Getting the Scoring Prediction Results ScoringPredictionValues result = scoringResult . getScoringPredictionValues (); Boolean anomaly = result . getPrediction (). equalsIgnoreCase ( \"Issue\" ); if (! anomaly ){ System . out . println ( \"No container anomaly\" ); message . ack (); // All processing of this message is done, ack it now return ReactiveStreams . empty (); } else { System . out . println ( \"A container anomaly has been predicted. Therefore, sending a ContainerAnomaly Event to the appropriate topic\" ); ContainerAnomalyEvent cae = new ContainerAnomalyEvent ( te . getContainerID (), te . getTimestamp (), t ); System . out . println ( \"ContainerAnomalyEvent object sent: \" + cae . toString ()); // This message will be sent on, create a new message which acknowledges the incoming message when it is acked return ReactiveStreams . of ( Message . of ( cae . toString (), () -> message . ack ())); } } where we are consuming/producing the data from/to Kafka by simply annotating our method with @Incoming and @Outgoing and leaving the rest of the magic required to create the reactive streams to the MicroProfile Reactive Messaging implementation of the OpenLiberty server does. Although there is certain magic in this process, there is still some configuration we need to provide OpenLiberty with in order for it to properly create the reactive streams. Such configuration is provided in the microprofile-configuration.properties file: # Config specific to reefer-telemetry kafka topic mp.messaging.incoming.reefer-telemetry.connector = liberty-kafka mp.messaging.incoming.reefer-telemetry.group.id = reefer-telemetry-reactive mp.messaging.incoming.reefer-telemetry.topic = reefer-telemetry mp.messaging.incoming.reefer-telemetry.key.deserializer = org.apache.kafka.common.serialization.StringDeserializer mp.messaging.incoming.reefer-telemetry.value.deserializer = org.apache.kafka.common.serialization.StringDeserializer # Config specific to containers kafka topic mp.messaging.outgoing.containers.connector = liberty-kafka mp.messaging.outgoing.containers.group.id = containers-reactive mp.messaging.outgoing.containers.topic = containers mp.messaging.outgoing.containers.key.serializer = org.apache.kafka.common.serialization.StringSerializer mp.messaging.outgoing.containers.value.serializer = org.apache.kafka.common.serialization.StringSerializer #### Config shared between all kafka connections # bootstrap server is the only config needed for plain insecure local kafka instance mp.messaging.connector.liberty-kafka.bootstrap.servers = where we can see the configuration for each of the incoming and outgoing reactive streams based on their names (reefer-telemetry and containers) that, in fact, match with the Kafka topics these reactive streams will be consuming/producing from/to. We also specify the serializer/deserializer we want MicroProfile Reactive Messaging to apply to our reactive streams for data processing but more importantly is the fact that we are telling OpenLiberty that the connector to be used is the liberty-kafka connector. Otherwise, OpenLiberty would not be able to interact with Kafka. Finally, we are providing to the liberty-kafka connector the appropriate configuration for OpenLiberty to successfully connect and consume/produce data to Kafka. In the example above we are providing the Kafka bootstrap server but there are other properties that can be set here (such as security) that can be found in the MicroProfile Reactive Messaging Specification","title":"Implement"},{"location":"infuse/dev-scoring/#build","text":"In order to be able to run our application, we first need to configure our OpenLiberty server to make use of the MicroProfile Reactive Messaging and MicroProfile Reactive Streams features it supports. Top do so, we specify such features in the server.xml file: <server description= \"Liberty server\" > <featureManager> <feature> microProfile-3.0 </feature> <!-- Reactive messaging and reactive streams --> <feature> mpReactiveMessaging-1.0 </feature> <feature> mpReactiveStreams-1.0 </feature> </featureManager> ... </server> Secondly, we need to configure our build process through the pom.xml file (we are using Maven to build our application) to compile the code and build the resulting application with: OpenLiberty as the target application server to run our application: <plugin> <groupId> net.wasdev.wlp.maven.plugins </groupId> <artifactId> liberty-maven-plugin </artifactId> <configuration> <assemblyArtifact> <groupId> ${liberty.groupId} </groupId> <artifactId> ${liberty.artifactId} </artifactId> <version> ${version.openliberty-runtime} </version> <type> zip </type> </assemblyArtifact> </configuration> </plugin> MicroProfile as the Java framework: <dependency> <groupId> org.eclipse.microprofile </groupId> <artifactId> microprofile </artifactId> <version> 3.0 </version> <type> pom </type> <scope> provided </scope> </dependency> MicroProfile Reactive Messaging and MicroProfile Reactive Streams capabilities: <dependency> <groupId> org.eclipse.microprofile.reactive.messaging </groupId> <artifactId> microprofile-reactive-messaging-api </artifactId> <version> 1.0 </version> <scope> provided </scope> </dependency> <dependency> <groupId> org.eclipse.microprofile.reactive-streams-operators </groupId> <artifactId> microprofile-reactive-streams-operators-api </artifactId> <version> 1.0.1 </version> <scope> provided </scope> </dependency> Apache Kafka Client: <dependency> <groupId> org.apache.kafka </groupId> <artifactId> kafka-clients </artifactId> <version> 2.3.0 </version> </dependency>","title":"Build"},{"location":"infuse/dev-scoring/#deploy","text":"Create a Kubernetes Secret for the required bootstrap.properties file which will pass the Kafka configuration details to the Reactive Messaging components. These values are the same as other existing ConfigMaps and Secrets, however they cannot be used in the same manner due to the order that they are required to be in the system when Liberty starts up. Download bootstrap.properties.tmpl Set mp.messaging.connector.liberty-kafka.bootstrap.servers to the comma-separated list of Kafka brokers. (This is the same value as the kafka-brokers ConfigMap.) Update mp.messaging.connector.liberty-kafka.sasl.jaas.config to replace the API-KEY placeholder value with the Event Streams API key. (This is the same value as the eventstreams-apikey Secret.) Create the Secret via the following command: kubectl create secret generic openliberty-bootstrap --from-file = bootstrap.properties = ./bootstrap.properties.tmpl Create a Kubernetes ConfigMap and Secret for the Cloud Pak For Data remote endpoints. ConfigMap: kubectl create configmap predictive-model-configmap --from-literal = baseURL = 'https://zen-cpd-zen.demo.ibmcloudpack.com/' --from-literal = predictionURL = '/v4/deployments/fb03738c-1234-abcd-wxyz-7e66106ee51c/predictions' Secret: kubectl create secret generic predictive-model-secret --from-literal = user = 'replace-with-your-username' --from-literal = password = 'replace-with-your-password' Create the necessary YAML files from the scoring-mp Helm Chart. A sample command is provided below: helm template --output-dir . --set serviceAccountName = reefer-simulator --set route.enabled = true chart/scoringmp kubectl apply -f ./scoringmp/template","title":"Deploy"},{"location":"infuse/dev-scoring/#run","text":"Once we have our application running, we can confirm the following by looking at the application logs: It is reacting to a data stream from the reefer-telemetry Kafka topic. It is calling the container anomaly predictive system. It is producing container anomaly events, when needed, into the containers Kafka topic. [AUDIT ] CWWKF0011I: The defaultServer server is ready to run a smarter planet. The defaultServer server started in 97.081 seconds. Received message: {\"containerID\": \"1111\", \"payload\": \"('1111', '2020-01-15 17:59:45', 'P05', 5.02702153, 5., 20.52035697, 2.62176459, 0, 1, 5, 21.56977522, 75.97754859, 39.85714797, 4.74727473, True, True, True, '37.8226902168957', '-122.324895', 0)\", \"timestamp\": \"2020-01-15 17:59:45\", \"type\": \"ReeferTelemetries\"} Getting Authentication token for the prediction service... IAM service returned: 200 Payload for the prediction service call: {\"input_data\":[{\"fields\":[\"temperature\",\"target_temperature\",\"ambiant_temperature\",\"kilowatts\",\"time_door_open\",\"defrost_cycle\",\"oxygen_level\",\"nitrogen_level\",\"humidity_level\",\"target_humidity_level\",\"carbon_dioxide_level\",\"fan_1\",\"fan_2\",\"fan_3\"],\"values\":[[5.02702153,5.0,20.52035697,2.62176459,0.0,5,21.56977522,75.97754859,39.85714797,39.85714797,4.74727473,true,true,true]]}]} This is the prediction: NoIssue This is the probability: [0.0010803937911987305,0.9989196062088013] No container anomaly Received message: {\"containerID\": \"1111\", \"payload\": \"('1111', '2020-01-15 17:59:45', 'P05', 5.02702153, 5., 20.52035697, 2.62176459, 0, 1, 5, 21.56977522, 75.97754859, 39.85714797, 4.74727473, True, True, True, '37.8226902168957', '-122.324895', 0)\", \"timestamp\": \"2020-01-15 17:59:45\", \"type\": \"ReeferTelemetries\"} Payload for the prediction service call: {\"input_data\":[{\"fields\":[\"temperature\",\"target_temperature\",\"ambiant_temperature\",\"kilowatts\",\"time_door_open\",\"defrost_cycle\",\"oxygen_level\",\"nitrogen_level\",\"humidity_level\",\"target_humidity_level\",\"carbon_dioxide_level\",\"fan_1\",\"fan_2\",\"fan_3\"],\"values\":[[5.02702153,5.0,20.52035697,2.62176459,0.0,5,21.56977522,75.97754859,39.85714797,39.85714797,4.74727473,true,true,true]]}]} This is the prediction: Issue This is the probability: [0.0010803937911987305,0.9989196062088013] A container anomaly has been predicted. Therefore, sending a ContainerAnomaly Event to the appropriate topic ContainerAnomalyEvent object sent: {containerID: 1111, timestamp: 1579111185000, type: ContainerAnomaly, payload: {temperature: 5.02702153, target_temperature: 5.0, ambiant_temperature: 20.52035697, kilowatts: 2.62176459, time_door_open: 0.0, content_type: 1, defrost_cycle: 5, oxygen_level: 21.56977522, nitrogen_level: 75.97754859, humidity_level: 39.85714797, carbon_dioxide_level: 4.74727473, fan_1: true, fan_2: true, fan_3: true, latitude: 37.8226902168957, longitude: -122.324895}}","title":"Run"},{"location":"infuse/dev-scoring/#deploying-the-model-using-watson-machine-learning","text":"TODO Cloud Pak model deployment","title":"Deploying the model using Watson Machine Learning"},{"location":"infuse/dev-scoring/#further-readings","text":"Appsody for cloud native development Appsody microprofile stack Cloud Pak for Application demo video Use Codewind for VScode Reactive messaging between microservices with MicroProfile on Open Liberty 19.0.0.9 Sending and receiving messages between microservices with MicroProfile Reactive Messaging MicroProfile Reactive Messaging Specification Reactive Messaging for MicroProfile Reactive Streams","title":"Further Readings"},{"location":"infuse/integration-tests/","text":"Integration tests to proof the solution Recall that the architecture of the deployed components look like in the figure below: So the first component to start is the container consumer which consumes events from the kafka containers topic. This topic is where the microservices will post messages about a Reefer container. It is used by this microservice already: Reefer container manager . Pre-requisites Be sure to have set the environment variables in the ./scripts/setenv.sh to point to your Event Stream or Kafka deployment. You need to start four terminal windows if you run the solution locally on you laptop, and only 2 terminals if you run the solution on our deployed cluster. Note Our deployed cluster in on IBM Cloud Openshift 3.11 cluster. Start Reefer container events consumer In the consumer folder use the command: ./runContainerConsumer.sh This script starts the docker python image, we built earlier and use the ConsumeContainers.py module. Start the predictive scoring service We can run it locally or on kubernetes cluster like Openshift. Under scoring folder, use the command: ./runScoringApp.sh In the beginning of the trace log you should see the bootstrap.servers brokers list, the group.id , and api key as sasl.password . Recalls the scoring is a producer and a consumer. See the build and run on Openshift section for running on kubernetes cluster. Start the simulator web app Under the simulator folder ./runReeferSimulator.sh To build and run it on Openshift review this section . Start a simulation Under the scripts folder ./sendSimulControl.sh Validate integration tests To test your local deployment ./sendSimulControl.sh localhost:8080 poweroff or to test on our cloud based deployed solution ./sendSimulControl.sh The traces will look like these: Simulator trace The trace from the pod demonstrate the configuration and the control message received at the POST operation, and then the event generated. {'bootstrap.servers': 'broker-3-\"<hidden-part>.eventstreams.cloud.ibm.com:9093,broker-1-\"<hidden-part>.eventstreams.cloud.ibm.com:9093,broker-0-\"<hidden-part>.eventstreams.cloud.ibm.com:9093,broker-5-\"<hidden-part>.eventstreams.cloud.ibm.com:9093,broker-2-\"<hidden-part>.eventstreams.cloud.ibm.com:9093,broker-4-\"<hidden-part>.eventstreams.cloud.ibm.com:9093', 'group.id': 'ReeferMetricsSimulator', 'security.protocol': 'sasl_ssl', 'sasl.mechanisms': 'PLAIN', 'ssl.ca.location': '/etc/pki/tls/cert.pem', 'sasl.username': 'token', 'sasl.password': '<hidden-part>'} ... {'containerID': 'c101', 'simulation': 'poweroff', 'nb_of_records': 50, 'good_temperature': 4.4} Reefer contaimer metric event to send:{\"containerID\": \"c101\", \"timestamp\": 1566859800, \"type\": \"ContainerMetric\", \"payload\": \"('2019-08-26 T22:50 Z', 'c101', 2.0905792037649547, 4.4, 16.282392569138707, 6.603341673152029, 2, 16, 8.827184272293419, 6.33603138958275, 0, 5)\"} Message delivered to containerMetrics [0] Reefer contaimer metric event to send:{\"containerID\": \"c101\", \"timestamp\": 1566859860, \"type\": \"ContainerMetric\", \"payload\": \"('2019-08-26 T22:51 Z', 'c101', 2.0905792037649547, 4.4, 0, -0.04371530981778182, 2, 3, 6.295683442800409, 5.36863196753292, 0, 1)\"} Message delivered to containerMetrics [0] ... Scoring trace Container consumer trace @@@ poll next container from containers partition: [0] at offset 3 with key b'c100': value: {\"timestamp\": 1566854815, \"type\": \"ContainerMaintenance\", \"version\": \"1\", \"containerID\": \"c100\", \"payload\": {\"containerID\": \"c100\", \"type\": \"Reefer\", \"status\": \"MaintenanceNeeded\", \"Reason\": \"Predictive maintenance scoring found a risk of failure\"}}","title":"Run integration tests"},{"location":"infuse/integration-tests/#integration-tests-to-proof-the-solution","text":"Recall that the architecture of the deployed components look like in the figure below: So the first component to start is the container consumer which consumes events from the kafka containers topic. This topic is where the microservices will post messages about a Reefer container. It is used by this microservice already: Reefer container manager .","title":"Integration tests to proof the solution"},{"location":"infuse/integration-tests/#pre-requisites","text":"Be sure to have set the environment variables in the ./scripts/setenv.sh to point to your Event Stream or Kafka deployment. You need to start four terminal windows if you run the solution locally on you laptop, and only 2 terminals if you run the solution on our deployed cluster. Note Our deployed cluster in on IBM Cloud Openshift 3.11 cluster.","title":"Pre-requisites"},{"location":"infuse/integration-tests/#start-reefer-container-events-consumer","text":"In the consumer folder use the command: ./runContainerConsumer.sh This script starts the docker python image, we built earlier and use the ConsumeContainers.py module.","title":"Start Reefer container events consumer"},{"location":"infuse/integration-tests/#start-the-predictive-scoring-service","text":"We can run it locally or on kubernetes cluster like Openshift. Under scoring folder, use the command: ./runScoringApp.sh In the beginning of the trace log you should see the bootstrap.servers brokers list, the group.id , and api key as sasl.password . Recalls the scoring is a producer and a consumer. See the build and run on Openshift section for running on kubernetes cluster.","title":"Start the predictive scoring service"},{"location":"infuse/integration-tests/#start-the-simulator-web-app","text":"Under the simulator folder ./runReeferSimulator.sh To build and run it on Openshift review this section .","title":"Start the simulator web app"},{"location":"infuse/integration-tests/#start-a-simulation","text":"Under the scripts folder ./sendSimulControl.sh","title":"Start a simulation"},{"location":"infuse/integration-tests/#validate-integration-tests","text":"To test your local deployment ./sendSimulControl.sh localhost:8080 poweroff or to test on our cloud based deployed solution ./sendSimulControl.sh The traces will look like these:","title":"Validate integration tests"},{"location":"infuse/integration-tests/#simulator-trace","text":"The trace from the pod demonstrate the configuration and the control message received at the POST operation, and then the event generated. {'bootstrap.servers': 'broker-3-\"<hidden-part>.eventstreams.cloud.ibm.com:9093,broker-1-\"<hidden-part>.eventstreams.cloud.ibm.com:9093,broker-0-\"<hidden-part>.eventstreams.cloud.ibm.com:9093,broker-5-\"<hidden-part>.eventstreams.cloud.ibm.com:9093,broker-2-\"<hidden-part>.eventstreams.cloud.ibm.com:9093,broker-4-\"<hidden-part>.eventstreams.cloud.ibm.com:9093', 'group.id': 'ReeferMetricsSimulator', 'security.protocol': 'sasl_ssl', 'sasl.mechanisms': 'PLAIN', 'ssl.ca.location': '/etc/pki/tls/cert.pem', 'sasl.username': 'token', 'sasl.password': '<hidden-part>'} ... {'containerID': 'c101', 'simulation': 'poweroff', 'nb_of_records': 50, 'good_temperature': 4.4} Reefer contaimer metric event to send:{\"containerID\": \"c101\", \"timestamp\": 1566859800, \"type\": \"ContainerMetric\", \"payload\": \"('2019-08-26 T22:50 Z', 'c101', 2.0905792037649547, 4.4, 16.282392569138707, 6.603341673152029, 2, 16, 8.827184272293419, 6.33603138958275, 0, 5)\"} Message delivered to containerMetrics [0] Reefer contaimer metric event to send:{\"containerID\": \"c101\", \"timestamp\": 1566859860, \"type\": \"ContainerMetric\", \"payload\": \"('2019-08-26 T22:51 Z', 'c101', 2.0905792037649547, 4.4, 0, -0.04371530981778182, 2, 3, 6.295683442800409, 5.36863196753292, 0, 1)\"} Message delivered to containerMetrics [0] ...","title":"Simulator trace"},{"location":"infuse/integration-tests/#scoring-trace","text":"","title":"Scoring trace"},{"location":"infuse/integration-tests/#container-consumer-trace","text":"@@@ poll next container from containers partition: [0] at offset 3 with key b'c100': value: {\"timestamp\": 1566854815, \"type\": \"ContainerMaintenance\", \"version\": \"1\", \"containerID\": \"c100\", \"payload\": {\"containerID\": \"c100\", \"type\": \"Reefer\", \"status\": \"MaintenanceNeeded\", \"Reason\": \"Predictive maintenance scoring found a risk of failure\"}}","title":"Container consumer trace"},{"location":"infuse/simul-app/","text":"The Simulator as web app The Simulator webapp is a simple python (3.7) Flask web app exposing a REST POST end point to control the type of simulation to run and to produce Reefer telemetry events to kafka reefer-telemetry topic. What_to_learn In this article we shortly present the design and implementation approaches used for this application, as well as how to use Appsody to jumpstart the implementation, and continously run and debug the application. We are presenting some best practice on TDD with python. Requirements: Job Stories The simulator is not in the critical path for production like component. It is here to help us develop the other components of the solution as we do not have real life Reefer container. To try something different, we are not using user stories to present what the simulator should do but we are using job stories . when I want to generate mockup telemetries data for my data scientist friend, I want to start the simulator tool from command line so I can get a csv file with data when I want to generate mockup telemetries data for my data scientist friend, I want to be able to simulate co2 sensor, o2 sensor and power sensor issue so I can get relevant data for the machine learning model to make sense when I want to generate mockup telemetries data for my data scientist friend, I want to start the simulator tool from command line using parameter so I can get save data to a remote document oriented database: mongodb on IBM cloud. when I want to demonstrate the solution, I want to call a REST api to control the generation of faulty sensor data so I can get the scoring service returning maintenance needed. The simulator needs to integrate with kafka / event stream deployed as service on the cloud or on-premise on openshift. Design approach To support remote control of the simulator while running as webapp, we define a POST operation on the /control URL: with a json control object to define the number records to simulate, the sensor to impact (co2sensor, o2sensor, power) , the container ID, (one of C01, C02, C03, C04) which carries the product referenced by product_id (one of P01, P02, P03, P04, P05): { 'containerID': 'C02', 'simulation': 'co2sensor', 'nb_of_records': 1000, \"product_id\" : \"P02\" } We have tried to support a domain driven design approach to structure the code, with domain, infrastructure and app modules. The domain module has a unique class for the simulator which main goals is to generate tuples or records for the different simulation types. It is reused for the standalone simulation tools to generate data at rest. As the simulator is also a webapp we need to package it with Flask and run it using one of the Web Server Gateway Interface (WSGI) implementation with Gunicorn . We recommend to follow Flask tutorial if you do not know this python library to develop web app or REST service. Flask is a simple library to implement REST based microservice and web application in python. It has other related projects to add interesting features to develop production application. The standard development includes defining routes, function to support handling the request and generating HTTP response, but also defining APIs... Read more with the explore Flask book online . Flask is mono threaded so it fits well in a simple web application for development purpose, but for production it is recommended to add a web server like Gunicorn to handle multiple concurrent requests. Code approach The application is built using Appsody as the developer experience tooling. The Appsody CLI is required locally to build and deploy the application properly. The app is done using Flask, and the code is generated using appsody init python-flask command with the Python Flask appsody stack and template. Appsody helps developer to do not worry about the details of k8s deployment and build. During a Appsody run, debug or test step (2), Appsody creates a Docker container based on the parent stack Dockerfile, and combines application code with the source code in the template. We recommend reading the Python Flask Appsody Stack git hub repo to get familiar with appsody python stack. This stack is defining the Flask application, and import the 'userapp' where the application code resides, then use blueprints to define health and metrics APIs: from flask import Flask app = Flask ( __name__ ) from userapp import * from server.routes.health import health_bp app . register_blueprint ( health_bp ) from server.routes.prometheus import metrics_bp app . register_blueprint ( metrics_bp ) This code is not updatable as it is part of the image. But we can add our business logic as part of the simulator/__init__.py code using another Flask blueprints module api/controller.py . The userapp module is defined when appsody integrates our code with the stack base image using Docker. Below is an extract of the docker file managing module installation and defining what appsody does during build, run and test: ENV APPSODY_MOUNTS=/:/project/userapp ENV APPSODY_DEPS=/project/deps WORKDIR /project RUN python -m pip install -r requirements.txt -t /project/deps ENV FLASK_APP=server/__init__.py Looking at the content of the final docker container running the application we can see this structure: /project |-- Dockerfile Pipfile Pipfile.lock constraints.txt requirements.txt deps/ server/ test/ userapp/ The basic concept of blueprints is that they record operations to execute when registered on an application. So to add the operation to support the control we add a blueprint, and then register it in the main application: __init__py . from userapp.api.controller import control_blueprint app . register_blueprint ( control_blueprint ) To define the API, we use Flasgger as an extension to Flask to extract Open API specification from the code. It comes with Swagger UI, so we can see the API documentation of the microservice at the URL /apidocs . It can also validate the data according to the schema defined. For the POST /control we define the using Swagger 2.0 the API in a separate file: api/controlapi.yml and import it at the method level to support the POSt operation. This method is defined in its blueprint as a REST resource. The code controller.py is under api folder. Below is a code extract to illustrate the use of Flask-RESTful and blueprint and the swagger annotation: from flasgger import swag_from from flask_restful import Resource , Api control_blueprint = Blueprint ( \"control\" , __name__ ) api = Api ( control_blueprint ) class SimulationController ( Resource ): @swag_from ( 'controlapi.yml' ) def post ( self ): # .. api . add_resource ( SimulationController , \"/control\" ) The Pipfile defines the dependencies for this component, and is used by pipenv during the automatic build process within appsody build . To launch the web application in development mode, using an IBM Event Streams remote use the following commands: # set environment variables - from simulator folder $ source ../scripts/setenv.sh OCP # Start appsody with the environment variables: in simulator folder $ appsody run --docker-options = \"-e KAFKA_BROKERS= $KAFKA_BROKERS -e KAFKA_APIKEY= $KAFKA_APIKEY -e KAFKA_CERT= $KAFKA_CERT -e TELEMETRY_TOPIC= $TELEMETRY_TOPIC -e CONTAINER_TOPIC= $CONTAINER_TOPIC \" The trace shows the Kafka configuration options: Kafka options are: [Container] {'bootstrap.servers': 'eventstream140-ibm-es-proxy-route-broker-0-eventstreams.apps.green.ocp.csplab.local:443', 'group.id': 'ReeferTelemetryProducers', 'security.protocol': 'SASL_SSL', 'sasl.mechanisms': 'PLAIN', 'sasl.username': 'token', 'sasl.password': 'xCByo4478xQH...EVdcbGNCtLLuItKgVDc', 'ssl.ca.location': '/project/userapp/certs/ocp/es-cert.pem'} Testing Unit test the Simulator The test coverage is not yet great. To run the test use appsody test . cd simulator . / startPythonEnv root @1 de81b16f940 : / # export PYTHONPATH =/ home / simulator root @1 de81b16f940 : / # cd / home / simulator root @1 de81b16f940 : / # python tests / unit / TestSimulator . py Functional testing Prepare for kubernetes deployment There are three required configuration elements for connectivity to IBM Event Streams (Kafka) prior to deployment as we already presented in the following notes. A ConfigMap named kafka-brokers Reference Link A Secret named eventstreams-api-key Reference Link A Secret named eventstreams-cert-pem (if connecting to an on-premise version of IBM Event Streams) Reference Link Once those elements are defined it is important to configure the app so it can retrieve those information via environment variables. With Appsody the file appsody-config.yaml is supporting these configurations. See the lines 24 to 40 to get the settings to map environment variables to Config Maps or Secrets. When using TLS connection, the TLS certificate is mounted inside the container via mounted file system. Inside the container the directory will be /certs and the volume is in fact a Secret containing the pem certificate as string. volumeMounts : - mountPath : /certs name : eventstreams-cert-pem volumes : - name : eventstreams-cert-pem secret : optional : true secretName : eventstreams-cert-pem Build The Docker image can be built from this directory by using the appsody build command: Ensure you are logged in to the desired remote Docker registry through your local Docker client. The appsody build -t ibmcase/kcontainer-reefer-simulator:appsody-v1 --push command will build and push the application's Docker image to the specified remote image registry (here this is the public docker hub repository). Application deployment The application can be deployed to a remote OpenShift cluster by using the appsody deploy command (We recommend reading Appsody build and deploy product documentation ): Deploy using the docker image on public docker hub repository: # login to the openshift cluster if not done already oc login --token = rR.... --server = https://api.green.ocp.csplab.local:6443 # Deploy to the eda-sandbox project appsody deploy -t ibmcase/kcontainer-reefer-simulator:appsody-v1 --namespace eda-sandbox --no-build You can verify the deployment with the CLI oc get pods or the via the Openshift console: To make the webapp visible externally to the cluster, you need to add a route for this deployment. Login to the admin console and use Create Route button on top right of the screen, Then enter a name and select the existing service Once created, the URL of the app is visible in the route list panel: Add the host name in your local /etc/hosts or be sure the hostname is defined in DNS server. Map to the IP address of the kubernetes proxy server end point. Continuous deployment with Tekton The general approach to use Tekton to deploy the components of the solution is defined in this note . Usage Once deployed, you can access the Swagger-based REST API via the defined route and trigger the simulation controls. To determine the route, use the oc get route reefer-simulator command and go to the URL specified in the HOST/PORT field in your browser. From there, drill down into the POST /control section and click Try it out! . Enter any of the following options for the fields prepopulated in the control body: Container: C01, C02, C03, C04 Product: P01, P02, P03, P04 Simulation: poweroff, co2sensor, o2sensor, normal Number of records: A positive integer Click Execute To generate data at rest The same simulator can be run as a standalone tool to create csv files or to write to a remote mongoDB database. The explanation on how to use this tool is done in a separate note here . More readings Flask Restful Appsody build and deploy product documentation","title":"Develop the simulation app"},{"location":"infuse/simul-app/#the-simulator-as-web-app","text":"The Simulator webapp is a simple python (3.7) Flask web app exposing a REST POST end point to control the type of simulation to run and to produce Reefer telemetry events to kafka reefer-telemetry topic. What_to_learn In this article we shortly present the design and implementation approaches used for this application, as well as how to use Appsody to jumpstart the implementation, and continously run and debug the application. We are presenting some best practice on TDD with python.","title":"The Simulator as web app"},{"location":"infuse/simul-app/#requirements-job-stories","text":"The simulator is not in the critical path for production like component. It is here to help us develop the other components of the solution as we do not have real life Reefer container. To try something different, we are not using user stories to present what the simulator should do but we are using job stories . when I want to generate mockup telemetries data for my data scientist friend, I want to start the simulator tool from command line so I can get a csv file with data when I want to generate mockup telemetries data for my data scientist friend, I want to be able to simulate co2 sensor, o2 sensor and power sensor issue so I can get relevant data for the machine learning model to make sense when I want to generate mockup telemetries data for my data scientist friend, I want to start the simulator tool from command line using parameter so I can get save data to a remote document oriented database: mongodb on IBM cloud. when I want to demonstrate the solution, I want to call a REST api to control the generation of faulty sensor data so I can get the scoring service returning maintenance needed. The simulator needs to integrate with kafka / event stream deployed as service on the cloud or on-premise on openshift.","title":"Requirements: Job Stories"},{"location":"infuse/simul-app/#design-approach","text":"To support remote control of the simulator while running as webapp, we define a POST operation on the /control URL: with a json control object to define the number records to simulate, the sensor to impact (co2sensor, o2sensor, power) , the container ID, (one of C01, C02, C03, C04) which carries the product referenced by product_id (one of P01, P02, P03, P04, P05): { 'containerID': 'C02', 'simulation': 'co2sensor', 'nb_of_records': 1000, \"product_id\" : \"P02\" } We have tried to support a domain driven design approach to structure the code, with domain, infrastructure and app modules. The domain module has a unique class for the simulator which main goals is to generate tuples or records for the different simulation types. It is reused for the standalone simulation tools to generate data at rest. As the simulator is also a webapp we need to package it with Flask and run it using one of the Web Server Gateway Interface (WSGI) implementation with Gunicorn . We recommend to follow Flask tutorial if you do not know this python library to develop web app or REST service. Flask is a simple library to implement REST based microservice and web application in python. It has other related projects to add interesting features to develop production application. The standard development includes defining routes, function to support handling the request and generating HTTP response, but also defining APIs... Read more with the explore Flask book online . Flask is mono threaded so it fits well in a simple web application for development purpose, but for production it is recommended to add a web server like Gunicorn to handle multiple concurrent requests.","title":"Design approach"},{"location":"infuse/simul-app/#code-approach","text":"The application is built using Appsody as the developer experience tooling. The Appsody CLI is required locally to build and deploy the application properly. The app is done using Flask, and the code is generated using appsody init python-flask command with the Python Flask appsody stack and template. Appsody helps developer to do not worry about the details of k8s deployment and build. During a Appsody run, debug or test step (2), Appsody creates a Docker container based on the parent stack Dockerfile, and combines application code with the source code in the template. We recommend reading the Python Flask Appsody Stack git hub repo to get familiar with appsody python stack. This stack is defining the Flask application, and import the 'userapp' where the application code resides, then use blueprints to define health and metrics APIs: from flask import Flask app = Flask ( __name__ ) from userapp import * from server.routes.health import health_bp app . register_blueprint ( health_bp ) from server.routes.prometheus import metrics_bp app . register_blueprint ( metrics_bp ) This code is not updatable as it is part of the image. But we can add our business logic as part of the simulator/__init__.py code using another Flask blueprints module api/controller.py . The userapp module is defined when appsody integrates our code with the stack base image using Docker. Below is an extract of the docker file managing module installation and defining what appsody does during build, run and test: ENV APPSODY_MOUNTS=/:/project/userapp ENV APPSODY_DEPS=/project/deps WORKDIR /project RUN python -m pip install -r requirements.txt -t /project/deps ENV FLASK_APP=server/__init__.py Looking at the content of the final docker container running the application we can see this structure: /project |-- Dockerfile Pipfile Pipfile.lock constraints.txt requirements.txt deps/ server/ test/ userapp/ The basic concept of blueprints is that they record operations to execute when registered on an application. So to add the operation to support the control we add a blueprint, and then register it in the main application: __init__py . from userapp.api.controller import control_blueprint app . register_blueprint ( control_blueprint ) To define the API, we use Flasgger as an extension to Flask to extract Open API specification from the code. It comes with Swagger UI, so we can see the API documentation of the microservice at the URL /apidocs . It can also validate the data according to the schema defined. For the POST /control we define the using Swagger 2.0 the API in a separate file: api/controlapi.yml and import it at the method level to support the POSt operation. This method is defined in its blueprint as a REST resource. The code controller.py is under api folder. Below is a code extract to illustrate the use of Flask-RESTful and blueprint and the swagger annotation: from flasgger import swag_from from flask_restful import Resource , Api control_blueprint = Blueprint ( \"control\" , __name__ ) api = Api ( control_blueprint ) class SimulationController ( Resource ): @swag_from ( 'controlapi.yml' ) def post ( self ): # .. api . add_resource ( SimulationController , \"/control\" ) The Pipfile defines the dependencies for this component, and is used by pipenv during the automatic build process within appsody build . To launch the web application in development mode, using an IBM Event Streams remote use the following commands: # set environment variables - from simulator folder $ source ../scripts/setenv.sh OCP # Start appsody with the environment variables: in simulator folder $ appsody run --docker-options = \"-e KAFKA_BROKERS= $KAFKA_BROKERS -e KAFKA_APIKEY= $KAFKA_APIKEY -e KAFKA_CERT= $KAFKA_CERT -e TELEMETRY_TOPIC= $TELEMETRY_TOPIC -e CONTAINER_TOPIC= $CONTAINER_TOPIC \" The trace shows the Kafka configuration options: Kafka options are: [Container] {'bootstrap.servers': 'eventstream140-ibm-es-proxy-route-broker-0-eventstreams.apps.green.ocp.csplab.local:443', 'group.id': 'ReeferTelemetryProducers', 'security.protocol': 'SASL_SSL', 'sasl.mechanisms': 'PLAIN', 'sasl.username': 'token', 'sasl.password': 'xCByo4478xQH...EVdcbGNCtLLuItKgVDc', 'ssl.ca.location': '/project/userapp/certs/ocp/es-cert.pem'}","title":"Code approach"},{"location":"infuse/simul-app/#testing","text":"","title":"Testing"},{"location":"infuse/simul-app/#unit-test-the-simulator","text":"The test coverage is not yet great. To run the test use appsody test . cd simulator . / startPythonEnv root @1 de81b16f940 : / # export PYTHONPATH =/ home / simulator root @1 de81b16f940 : / # cd / home / simulator root @1 de81b16f940 : / # python tests / unit / TestSimulator . py","title":"Unit test the Simulator"},{"location":"infuse/simul-app/#functional-testing","text":"","title":"Functional testing"},{"location":"infuse/simul-app/#prepare-for-kubernetes-deployment","text":"There are three required configuration elements for connectivity to IBM Event Streams (Kafka) prior to deployment as we already presented in the following notes. A ConfigMap named kafka-brokers Reference Link A Secret named eventstreams-api-key Reference Link A Secret named eventstreams-cert-pem (if connecting to an on-premise version of IBM Event Streams) Reference Link Once those elements are defined it is important to configure the app so it can retrieve those information via environment variables. With Appsody the file appsody-config.yaml is supporting these configurations. See the lines 24 to 40 to get the settings to map environment variables to Config Maps or Secrets. When using TLS connection, the TLS certificate is mounted inside the container via mounted file system. Inside the container the directory will be /certs and the volume is in fact a Secret containing the pem certificate as string. volumeMounts : - mountPath : /certs name : eventstreams-cert-pem volumes : - name : eventstreams-cert-pem secret : optional : true secretName : eventstreams-cert-pem","title":"Prepare for kubernetes deployment"},{"location":"infuse/simul-app/#build","text":"The Docker image can be built from this directory by using the appsody build command: Ensure you are logged in to the desired remote Docker registry through your local Docker client. The appsody build -t ibmcase/kcontainer-reefer-simulator:appsody-v1 --push command will build and push the application's Docker image to the specified remote image registry (here this is the public docker hub repository).","title":"Build"},{"location":"infuse/simul-app/#application-deployment","text":"The application can be deployed to a remote OpenShift cluster by using the appsody deploy command (We recommend reading Appsody build and deploy product documentation ): Deploy using the docker image on public docker hub repository: # login to the openshift cluster if not done already oc login --token = rR.... --server = https://api.green.ocp.csplab.local:6443 # Deploy to the eda-sandbox project appsody deploy -t ibmcase/kcontainer-reefer-simulator:appsody-v1 --namespace eda-sandbox --no-build You can verify the deployment with the CLI oc get pods or the via the Openshift console: To make the webapp visible externally to the cluster, you need to add a route for this deployment. Login to the admin console and use Create Route button on top right of the screen, Then enter a name and select the existing service Once created, the URL of the app is visible in the route list panel: Add the host name in your local /etc/hosts or be sure the hostname is defined in DNS server. Map to the IP address of the kubernetes proxy server end point.","title":"Application deployment"},{"location":"infuse/simul-app/#continuous-deployment-with-tekton","text":"The general approach to use Tekton to deploy the components of the solution is defined in this note .","title":"Continuous deployment with Tekton"},{"location":"infuse/simul-app/#usage","text":"Once deployed, you can access the Swagger-based REST API via the defined route and trigger the simulation controls. To determine the route, use the oc get route reefer-simulator command and go to the URL specified in the HOST/PORT field in your browser. From there, drill down into the POST /control section and click Try it out! . Enter any of the following options for the fields prepopulated in the control body: Container: C01, C02, C03, C04 Product: P01, P02, P03, P04 Simulation: poweroff, co2sensor, o2sensor, normal Number of records: A positive integer Click Execute","title":"Usage"},{"location":"infuse/simul-app/#to-generate-data-at-rest","text":"The same simulator can be run as a standalone tool to create csv files or to write to a remote mongoDB database. The explanation on how to use this tool is done in a separate note here .","title":"To generate data at rest"},{"location":"infuse/simul-app/#more-readings","text":"Flask Restful Appsody build and deploy product documentation","title":"More readings"}]}